{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rasa NLU Examples \u00b6 This repository contains some example components meant for educational and inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests and some documentation but this is a community project, not something that is part of core Rasa. The goal of these tools will be to be compatible with the most recent version of rasa only. You may need to point to an older release of the project if you want it to be compatible with an older version of Rasa. The following components are implemented. Tokenizers \u00b6 Tokenizers can split up the input text into tokens. Depending on the Tokenizer that you pick you can also choose to apply lemmatization. For languages that have rich grammatical features this might help reduce the size of all the possible tokens. StanzaTokenizer \u00b6 rasa_nlu_examples.tokenizers.StanzaTokenizer docs We support a tokenizier based on Stanza . This tokenizer offers part of speech tagging as well as lemmatization for many languages that spaCy currently does not support. These features might help your ML pipelines in those situations. ThaiTokenizer \u00b6 rasa_nlu_examples.tokenizers.ThaiTokenizer docs We support a Thai tokenizier based on PyThaiNLP link . Dense Featurizers \u00b6 Dense featurizers attach dense numeric features per token as well as to the entire utterance. These features are picked up by intent classifiers and entity detectors later in the pipeline. FastTextFeaturizer \u00b6 rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs These are the pretrained embeddings from FastText, see for more info here . These are available in 157 languages, see here . BytePairFeaturizer \u00b6 rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs These BytePair embeddings are specialized subword embeddings that are built to be lightweight. See this link for more information. These are available in 227 languages and you can specify the subword vocabulary size as well as the dimensionality. GensimFeaturizer \u00b6 rasa_nlu_examples.featurizers.dense.GensimFeaturizer docs A benefit of the gensim library is that it is very easy to train your own word embeddings. It's typically only about 5 lines of code. That means that you could train your own word-embeddings and then easily use them in a Rasa pipeline. This can be useful if you have specific jargon you'd like to capture. Another benefit of the tool is that it has made it easy for community members to train custom embeddings for many languages. Here's a list of resources; AraVec has embeddings for Arabic trained on twitter and/or Wikipedia. Fallback Classifiers \u00b6 Fallback classifiers are models that can override previous intents. In Rasa NLU there is a NLU Fallback Classifier that can \"fallback\" whenever the main classifier isn't confident about their prediction. In this repository we also host a few of these models such that you can handle specific instances with a custom model too. These models are meant to be used in combination with a RulePolicy . FasttextLanguage \u00b6 rasa_nlu_examples.fallback.FasttextLanguageFallbackClassifier docs This fallback classifier is based on fasttext . It can detect when a user is speaking in an unintended language such that you can create a rule to respond appropriately. Meta \u00b6 The components listed here won't effect the NLU pipeline but they might instead cause extra logs to appear to help with debugging. Printer \u00b6 rasa_nlu_examples.meta.Printer docs This component will print what each featurizer adds to the NLU message. Very useful for debugging. You can find a tutorial on it here . Entity Extraction \u00b6 Name Lists \u00b6 Language models in spaCy are typically trained on Western news datasets. That means that the reported benchmarks might not apply to your use-case. For example; detecting names in texts from France is not the same thing as detecting names in Madagascar. Even thought French is used actively in both countries, the names of it's citizens might be so different that you cannot assume that the benchmarks apply universally. To remedy this we've started collecting name lists. These can be used as a lookup table which can be picked up by Rasa's RegexEntityExtractor . It won't be 100% perfect but it should give a reasonable starting point. You can find the namelists here . We currently offer namelists for the United States, Germany as well as common Arabic names. Feel free to submit PRs for more languages. We're also eager to receive feedback. Contributing \u00b6 You can find the contribution guide here .","title":"Home"},{"location":"#rasa-nlu-examples","text":"This repository contains some example components meant for educational and inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests and some documentation but this is a community project, not something that is part of core Rasa. The goal of these tools will be to be compatible with the most recent version of rasa only. You may need to point to an older release of the project if you want it to be compatible with an older version of Rasa. The following components are implemented.","title":"Rasa NLU Examples"},{"location":"#tokenizers","text":"Tokenizers can split up the input text into tokens. Depending on the Tokenizer that you pick you can also choose to apply lemmatization. For languages that have rich grammatical features this might help reduce the size of all the possible tokens.","title":"Tokenizers"},{"location":"#stanzatokenizer","text":"rasa_nlu_examples.tokenizers.StanzaTokenizer docs We support a tokenizier based on Stanza . This tokenizer offers part of speech tagging as well as lemmatization for many languages that spaCy currently does not support. These features might help your ML pipelines in those situations.","title":"StanzaTokenizer"},{"location":"#thaitokenizer","text":"rasa_nlu_examples.tokenizers.ThaiTokenizer docs We support a Thai tokenizier based on PyThaiNLP link .","title":"ThaiTokenizer"},{"location":"#dense-featurizers","text":"Dense featurizers attach dense numeric features per token as well as to the entire utterance. These features are picked up by intent classifiers and entity detectors later in the pipeline.","title":"Dense Featurizers"},{"location":"#fasttextfeaturizer","text":"rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs These are the pretrained embeddings from FastText, see for more info here . These are available in 157 languages, see here .","title":"FastTextFeaturizer"},{"location":"#bytepairfeaturizer","text":"rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs These BytePair embeddings are specialized subword embeddings that are built to be lightweight. See this link for more information. These are available in 227 languages and you can specify the subword vocabulary size as well as the dimensionality.","title":"BytePairFeaturizer"},{"location":"#gensimfeaturizer","text":"rasa_nlu_examples.featurizers.dense.GensimFeaturizer docs A benefit of the gensim library is that it is very easy to train your own word embeddings. It's typically only about 5 lines of code. That means that you could train your own word-embeddings and then easily use them in a Rasa pipeline. This can be useful if you have specific jargon you'd like to capture. Another benefit of the tool is that it has made it easy for community members to train custom embeddings for many languages. Here's a list of resources; AraVec has embeddings for Arabic trained on twitter and/or Wikipedia.","title":"GensimFeaturizer"},{"location":"#fallback-classifiers","text":"Fallback classifiers are models that can override previous intents. In Rasa NLU there is a NLU Fallback Classifier that can \"fallback\" whenever the main classifier isn't confident about their prediction. In this repository we also host a few of these models such that you can handle specific instances with a custom model too. These models are meant to be used in combination with a RulePolicy .","title":"Fallback Classifiers"},{"location":"#fasttextlanguage","text":"rasa_nlu_examples.fallback.FasttextLanguageFallbackClassifier docs This fallback classifier is based on fasttext . It can detect when a user is speaking in an unintended language such that you can create a rule to respond appropriately.","title":"FasttextLanguage"},{"location":"#meta","text":"The components listed here won't effect the NLU pipeline but they might instead cause extra logs to appear to help with debugging.","title":"Meta"},{"location":"#printer","text":"rasa_nlu_examples.meta.Printer docs This component will print what each featurizer adds to the NLU message. Very useful for debugging. You can find a tutorial on it here .","title":"Printer"},{"location":"#entity-extraction","text":"","title":"Entity Extraction"},{"location":"#name-lists","text":"Language models in spaCy are typically trained on Western news datasets. That means that the reported benchmarks might not apply to your use-case. For example; detecting names in texts from France is not the same thing as detecting names in Madagascar. Even thought French is used actively in both countries, the names of it's citizens might be so different that you cannot assume that the benchmarks apply universally. To remedy this we've started collecting name lists. These can be used as a lookup table which can be picked up by Rasa's RegexEntityExtractor . It won't be 100% perfect but it should give a reasonable starting point. You can find the namelists here . We currently offer namelists for the United States, Germany as well as common Arabic names. Feel free to submit PRs for more languages. We're also eager to receive feedback.","title":"Name Lists"},{"location":"#contributing","text":"You can find the contribution guide here .","title":"Contributing"},{"location":"benchmarking/","text":"Benchmarking Guide \u00b6 This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files with NLU components from this library. You can glance over some examples below. Basic Config Here's a very basic configuartion file. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: DIETClassifier epochs: 200 Assuming this file is named basic-config.yml you can run this pipeline as a benchmark by running this command from the project directory; rasa test nlu --config basic-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-config This will generate output in the gridresults/basic-config folder. Basic Byte-Pair Here's the same basic configuration but now with dense features added. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 1000 dim: 25 - name: DIETClassifier epochs: 200 Assuming this file is named basic-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config basic-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-bytepair-config This will generate output in the gridresults/basic-bytepair-config folder. Medium Byte-Pair We've now increased the vocabulary size and dimensionality. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 10000 dim: 100 - name: DIETClassifier epochs: 200 Assuming this file is named medium-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config medium-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/medium-bytepair-config This will generate output in the gridresults/medium-bytepair-config folder. Large Byte-Pair We've now grabbed the largest English Byte-Pair embeddings available. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 200000 dim: 300 - name: DIETClassifier epochs: 200 Assuming this file is named large-bytepair-config.yml you can run this benchmark by running this command from the project directory; rasa test nlu --config large-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/large-bytepair-config This will generate output in the gridresults/large-bytepair-config folder. Final Reminder \u00b6 We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline. The components that we host here may very well lag behind Rasa Open Source too.","title":"Benchmarking Guide"},{"location":"benchmarking/#benchmarking-guide","text":"This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files with NLU components from this library. You can glance over some examples below. Basic Config Here's a very basic configuartion file. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: DIETClassifier epochs: 200 Assuming this file is named basic-config.yml you can run this pipeline as a benchmark by running this command from the project directory; rasa test nlu --config basic-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-config This will generate output in the gridresults/basic-config folder. Basic Byte-Pair Here's the same basic configuration but now with dense features added. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 1000 dim: 25 - name: DIETClassifier epochs: 200 Assuming this file is named basic-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config basic-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-bytepair-config This will generate output in the gridresults/basic-bytepair-config folder. Medium Byte-Pair We've now increased the vocabulary size and dimensionality. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 10000 dim: 100 - name: DIETClassifier epochs: 200 Assuming this file is named medium-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config medium-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/medium-bytepair-config This will generate output in the gridresults/medium-bytepair-config folder. Large Byte-Pair We've now grabbed the largest English Byte-Pair embeddings available. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 200000 dim: 300 - name: DIETClassifier epochs: 200 Assuming this file is named large-bytepair-config.yml you can run this benchmark by running this command from the project directory; rasa test nlu --config large-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/large-bytepair-config This will generate output in the gridresults/large-bytepair-config folder.","title":"Benchmarking Guide"},{"location":"benchmarking/#final-reminder","text":"We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline. The components that we host here may very well lag behind Rasa Open Source too.","title":"Final Reminder"},{"location":"contributing/","text":"Contributing Guide \u00b6 Ways you can Contribute \u00b6 We're open to contributions and there are many ways that you can make one. You can suggest new features. You can help review new features. You can submit new components. You can let us know if there are bugs. You can let us know if the components in this library help you. Open an Issue \u00b6 You can submit an issue here . Issues allow us to keep track of a conversation about this repository and it is the preferred communication channel for bugs related to this project. Suggest a New Feature \u00b6 This project started because we wanted to offer support for word embeddings for more languages. The first feature we added was support for FastText, which offers embeddings for 157 languages. We later received a contribution from a community member for BytePair embeddings, which offers support for 275 languages. We weren't aware of these embeddings but we were exited to support more languages. Odds are that there are many more tools out there that the maintainers of this project aren't aware of yet. There may very well be more embeddings, tokenziers, lemmatizers and models that we're not aware of but can help Rasa developers make better assistants. The goal of this project is to support more of these sorts of tools for Rasa users. You can help out the project just by letting us know if there's an integration missing. If you do not have the time to contribute a component yourself then you can still contribute to the effort by letting us know what components might help you make a better assistant. Share an Observation \u00b6 If the tools that we offer here turn out to be useful then we'd love to hear about it. We're also interested in hearing if these tools don't work for your usecase. Any feedback will be shared with the research team at Rasa. We're especially keen to hear feedback on the performance of the word embeddings that we host here. You can leave a message either on the github issue list or on the Rasa forum . Be sure to ping koaning on the forum if you mention this project, he's the main maintainer. Adding a new Component \u00b6 There's a balance between allowing experimentation and maintaining all the code. This is why we've come up with a checklist that you should keep in mind before you're submitting code. If you want to contribute a new component please make an issue first so we can discuss it. We want to prevent double work where possible and make sure the proposed component is appropriate for this repository. New tools that are added here need to be plausibly useful in a real life scenario. For example, we won't be able to accept a component that adds noise to the features. Think about unit tests. We prefer to standardise unit tests as much as possible but there may be specific things you'd like to check for. Ask for help! Feel free to ping the koaning on this repository if you're having trouble with an implementation or appreciate guidance on a topic. He'll gladly help you. Testing \u00b6 We run automated tests via GitHub actions but you can also run all the checking mechanisms locally. To run the tests locally you'll need to run a script beforehand. python tests/prepare_everything.py This will prepare the filesystem for testing. We do this to prevent the need of downloading very large word embeddings locally and in CI. Fasttext can be 6-7 GB and we don't want to pull such a payload at every CI step. You can also prepare files locally by installing all dependencies via the Makefile . make install You can also run all style and type checking mechanisms locally via the Makefile . make check","title":"Contributing"},{"location":"contributing/#contributing-guide","text":"","title":"Contributing Guide"},{"location":"contributing/#ways-you-can-contribute","text":"We're open to contributions and there are many ways that you can make one. You can suggest new features. You can help review new features. You can submit new components. You can let us know if there are bugs. You can let us know if the components in this library help you.","title":"Ways you can Contribute"},{"location":"contributing/#open-an-issue","text":"You can submit an issue here . Issues allow us to keep track of a conversation about this repository and it is the preferred communication channel for bugs related to this project.","title":"Open an Issue"},{"location":"contributing/#suggest-a-new-feature","text":"This project started because we wanted to offer support for word embeddings for more languages. The first feature we added was support for FastText, which offers embeddings for 157 languages. We later received a contribution from a community member for BytePair embeddings, which offers support for 275 languages. We weren't aware of these embeddings but we were exited to support more languages. Odds are that there are many more tools out there that the maintainers of this project aren't aware of yet. There may very well be more embeddings, tokenziers, lemmatizers and models that we're not aware of but can help Rasa developers make better assistants. The goal of this project is to support more of these sorts of tools for Rasa users. You can help out the project just by letting us know if there's an integration missing. If you do not have the time to contribute a component yourself then you can still contribute to the effort by letting us know what components might help you make a better assistant.","title":"Suggest a New Feature"},{"location":"contributing/#share-an-observation","text":"If the tools that we offer here turn out to be useful then we'd love to hear about it. We're also interested in hearing if these tools don't work for your usecase. Any feedback will be shared with the research team at Rasa. We're especially keen to hear feedback on the performance of the word embeddings that we host here. You can leave a message either on the github issue list or on the Rasa forum . Be sure to ping koaning on the forum if you mention this project, he's the main maintainer.","title":"Share an Observation"},{"location":"contributing/#adding-a-new-component","text":"There's a balance between allowing experimentation and maintaining all the code. This is why we've come up with a checklist that you should keep in mind before you're submitting code. If you want to contribute a new component please make an issue first so we can discuss it. We want to prevent double work where possible and make sure the proposed component is appropriate for this repository. New tools that are added here need to be plausibly useful in a real life scenario. For example, we won't be able to accept a component that adds noise to the features. Think about unit tests. We prefer to standardise unit tests as much as possible but there may be specific things you'd like to check for. Ask for help! Feel free to ping the koaning on this repository if you're having trouble with an implementation or appreciate guidance on a topic. He'll gladly help you.","title":"Adding a new Component"},{"location":"contributing/#testing","text":"We run automated tests via GitHub actions but you can also run all the checking mechanisms locally. To run the tests locally you'll need to run a script beforehand. python tests/prepare_everything.py This will prepare the filesystem for testing. We do this to prevent the need of downloading very large word embeddings locally and in CI. Fasttext can be 6-7 GB and we don't want to pull such a payload at every CI step. You can also prepare files locally by installing all dependencies via the Makefile . make install You can also run all style and type checking mechanisms locally via the Makefile . make check","title":"Testing"},{"location":"docs/fallback/fasttextlanguagefallback/","text":"FasttextLanguageFallbackClassifier \u00b6 This classifier uses fasttext to detect if an unintended language is used. You can combine this tool together with RulePolicy rules to handle out of scope responses more elegantly. Assuming that you're making an assistant to handle English then you can send the user an appropriate response if this model predicts another language. The tool should be able to detect 176 languages but the predictions won't be perfect. Especially when the user sends short utterances we need to be careful. That is why this tool allows you to specify a minimum number of characters and tokens before this model triggers an intent. Understanding the Tool \u00b6 You're encouraged to play with the tool from a jupyter notebook so you can understand what kinds of mistakes the model might make. To do that you'll first want to download the fasttext library as well as the pretrained model. python -m pip install fasttext wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz Then from a notebook you should be able to interact with the model by running: import fasttext model = fasttext . load_model ( \"lid.176.ftz\" ) txt = \"i am speaking english\" proba_dict = { k : v for k , v in zip ( * model . predict ( txt , k = 10 ))} proba_dict Configurable Variables \u00b6 expected_language (required): the language that you expect to be predicted. If this language is predicted we won't trigger an intent. intent_triggered (required): the name of the intent to trigger when the model does not detect the expected language. cache_dir (required): specifies the folder where the pretrained model can be found. model_file (required): specifies the path to a pretrained model file, typically you'll want lid.176.ftz . See the fasttext docs for more info. threshold (default: 0.7): if the probability for your language is smaller than this threshold then we trigger the intent. min_tokens (default: 3): the minimum number of tokens that need to be in the utterance. If there's less tokens the language model is ignored because it is likely to be in-accurate. min_chars (default: 10): the minimum number of characters of text that need to be in the utterance. If there's less tokens the language model is ignored because it is likely to be in-accurate. protected_intents (default: [] ): specifies a list of intent names that won't be overwritten Base Usage \u00b6 The configuration file below demonstrates how you might use the this component. In this example we've assumed that you've downloaded the lightweight \"lid.176.ftz\" model beforehand and that it exists in the downloaded folder that is on the root path of your project. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 1 - name : rasa_nlu_examples.fallback.FasttextLanguageFallbackClassifier expected_language : en intent_triggered : non_english cache_dir : downloaded file : 'lid.176.ftz' min_chars : 5 min_tokens : 2 threshold : 0.3 protected_intents : [ \"greet\" ] To get the most out of this tool you also need to add a rule to a rules.yml file. That way you can configure an appropriate action whenever a user is speaking in the wrong language. That might look something like this: rules : - rule : Ask the user to switch to speaking in English. steps : - intent : non_english - action : utter_non_english For more information on rules, see the docs .","title":"FasttextLanguage"},{"location":"docs/fallback/fasttextlanguagefallback/#fasttextlanguagefallbackclassifier","text":"This classifier uses fasttext to detect if an unintended language is used. You can combine this tool together with RulePolicy rules to handle out of scope responses more elegantly. Assuming that you're making an assistant to handle English then you can send the user an appropriate response if this model predicts another language. The tool should be able to detect 176 languages but the predictions won't be perfect. Especially when the user sends short utterances we need to be careful. That is why this tool allows you to specify a minimum number of characters and tokens before this model triggers an intent.","title":"FasttextLanguageFallbackClassifier"},{"location":"docs/fallback/fasttextlanguagefallback/#understanding-the-tool","text":"You're encouraged to play with the tool from a jupyter notebook so you can understand what kinds of mistakes the model might make. To do that you'll first want to download the fasttext library as well as the pretrained model. python -m pip install fasttext wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz Then from a notebook you should be able to interact with the model by running: import fasttext model = fasttext . load_model ( \"lid.176.ftz\" ) txt = \"i am speaking english\" proba_dict = { k : v for k , v in zip ( * model . predict ( txt , k = 10 ))} proba_dict","title":"Understanding the Tool"},{"location":"docs/fallback/fasttextlanguagefallback/#configurable-variables","text":"expected_language (required): the language that you expect to be predicted. If this language is predicted we won't trigger an intent. intent_triggered (required): the name of the intent to trigger when the model does not detect the expected language. cache_dir (required): specifies the folder where the pretrained model can be found. model_file (required): specifies the path to a pretrained model file, typically you'll want lid.176.ftz . See the fasttext docs for more info. threshold (default: 0.7): if the probability for your language is smaller than this threshold then we trigger the intent. min_tokens (default: 3): the minimum number of tokens that need to be in the utterance. If there's less tokens the language model is ignored because it is likely to be in-accurate. min_chars (default: 10): the minimum number of characters of text that need to be in the utterance. If there's less tokens the language model is ignored because it is likely to be in-accurate. protected_intents (default: [] ): specifies a list of intent names that won't be overwritten","title":"Configurable Variables"},{"location":"docs/fallback/fasttextlanguagefallback/#base-usage","text":"The configuration file below demonstrates how you might use the this component. In this example we've assumed that you've downloaded the lightweight \"lid.176.ftz\" model beforehand and that it exists in the downloaded folder that is on the root path of your project. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 1 - name : rasa_nlu_examples.fallback.FasttextLanguageFallbackClassifier expected_language : en intent_triggered : non_english cache_dir : downloaded file : 'lid.176.ftz' min_chars : 5 min_tokens : 2 threshold : 0.3 protected_intents : [ \"greet\" ] To get the most out of this tool you also need to add a rule to a rules.yml file. That way you can configure an appropriate action whenever a user is speaking in the wrong language. That might look something like this: rules : - rule : Ask the user to switch to speaking in English. steps : - intent : non_english - action : utter_non_english For more information on rules, see the docs .","title":"Base Usage"},{"location":"docs/featurizer/bytepair/","text":"This featurizer is a dense featurizer. If you're interested in learning how these work you might appreciate reading the original article . Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. A main feature of these types of embeddings is that they are relatively lightweight but also their availability in many languages. BytePair embeddings exist for 277 languages that are pretrained on wikipedia. There's also availability for a multi-language setting. More information on these embeddings can be found here . When you scroll down you will notice a large of languages that are available. Here's some examples from that list that give a detailed view of available vectors: Abkhazian Zulu English Hindi Chinese Esperanto Multi Language Configurable Variables \u00b6 lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25 , vs : specifies the vocabulary size of the segmentation model, default = 1000 , vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default= True cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default= None , emb_file : specifies the path to a custom embedding file, default= None Base Usage \u00b6 The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100 Cached Usage \u00b6 If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 Note that in this case we expect two files to be present in the tests/data directory; en.wiki.bpe.vs10000.d100.w2v.bin en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file and emb_file settings. But it is preferable to stick to the library naming convention. Also note that if you use the model_file and emb_file settings that you must provide full filepaths and that the cache_dir will be ignored. It is still considered good practice to manually specify the lang , dim and vs parameter in this situation.","title":"BytePairFeaturizer"},{"location":"docs/featurizer/bytepair/#configurable-variables","text":"lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25 , vs : specifies the vocabulary size of the segmentation model, default = 1000 , vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default= True cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default= None , emb_file : specifies the path to a custom embedding file, default= None","title":"Configurable Variables"},{"location":"docs/featurizer/bytepair/#base-usage","text":"The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/bytepair/#cached-usage","text":"If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 Note that in this case we expect two files to be present in the tests/data directory; en.wiki.bpe.vs10000.d100.w2v.bin en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file and emb_file settings. But it is preferable to stick to the library naming convention. Also note that if you use the model_file and emb_file settings that you must provide full filepaths and that the cache_dir will be ignored. It is still considered good practice to manually specify the lang , dim and vs parameter in this situation.","title":"Cached Usage"},{"location":"docs/featurizer/fasttext/","text":"Fasttext supports word embeddings for 157 languages and is trained on both Common Crawl and Wikipedia. You can download the embeddings here . Note that this featurizer is a dense featurizer. Beware that these embedding files tend to be big: about 6-7Gb. Configurable Variables \u00b6 cache_dir : pass it the name of the directory where you've downloaded the embeddings file : pass it the name of the file that contains the word embeddings Base Usage \u00b6 The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_dir : downloaded/beforehand file : cc.nl.300.bin - name : DIETClassifier epochs : 100","title":"FastTextFeaturizer"},{"location":"docs/featurizer/fasttext/#configurable-variables","text":"cache_dir : pass it the name of the directory where you've downloaded the embeddings file : pass it the name of the file that contains the word embeddings","title":"Configurable Variables"},{"location":"docs/featurizer/fasttext/#base-usage","text":"The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_dir : downloaded/beforehand file : cc.nl.300.bin - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/gensim/","text":"This page discusses some properties of the GensimFeaturizer . Note that this featurizer is a dense featurizer. Gensim is a popular python library that makes it relatively easy to train your own word vectors. This can be useful if your corpus is very different than what most popular embeddings are trained on. We'll give a small guide on how to train your own embeddings here but you can also read the guide on the gensim docs . Training Your Own \u00b6 Training your own gensim model can be done in a few lines of code. A demonstration is shown below. from gensim.models import Word2Vec # Gensim needs a list of lists to represent tokens in a document. # In real life you\u2019d read a text file and turn it into lists here. text = [ \"this is a sentence\" , \"so is this\" , \"and we're all talking\" ] tokens = [ t . split ( \" \" ) for t in text ] # This is where we train new word embeddings. model = Word2Vec ( sentences = tokens , size = 10 , window = 3 , min_count = 1 , iter = 5 , workers = 2 ) # This is where they are saved to disk. model . wv . save ( \"wordvectors.kv\" ) This wordvectors.kv file should contain all the vectors that you've trained. It's this file that you can pass on to this component. Configurable Variables \u00b6 cache_dir : pass it the name of the directory where you've downloaded/saved the embeddings file : pass it the name of the .kv file that contains the word embeddings Base Usage \u00b6 The configuration file below demonstrates how you might use the gensim embeddings. In this example we're building a pipeline for the an English language and we're assuming that you've trained your own embeddings which have been saved upfront as saved/beforehand/filename.kv . language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.GensimFeaturizer cache_dir : saved/beforehand file : filename.kv - name : DIETClassifier epochs : 100","title":"GensimFeaturizer"},{"location":"docs/featurizer/gensim/#training-your-own","text":"Training your own gensim model can be done in a few lines of code. A demonstration is shown below. from gensim.models import Word2Vec # Gensim needs a list of lists to represent tokens in a document. # In real life you\u2019d read a text file and turn it into lists here. text = [ \"this is a sentence\" , \"so is this\" , \"and we're all talking\" ] tokens = [ t . split ( \" \" ) for t in text ] # This is where we train new word embeddings. model = Word2Vec ( sentences = tokens , size = 10 , window = 3 , min_count = 1 , iter = 5 , workers = 2 ) # This is where they are saved to disk. model . wv . save ( \"wordvectors.kv\" ) This wordvectors.kv file should contain all the vectors that you've trained. It's this file that you can pass on to this component.","title":"Training Your Own"},{"location":"docs/featurizer/gensim/#configurable-variables","text":"cache_dir : pass it the name of the directory where you've downloaded/saved the embeddings file : pass it the name of the .kv file that contains the word embeddings","title":"Configurable Variables"},{"location":"docs/featurizer/gensim/#base-usage","text":"The configuration file below demonstrates how you might use the gensim embeddings. In this example we're building a pipeline for the an English language and we're assuming that you've trained your own embeddings which have been saved upfront as saved/beforehand/filename.kv . language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.GensimFeaturizer cache_dir : saved/beforehand file : filename.kv - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/jupyter/tools/","text":"This library also hosts some common tools to make it easier to investigate your training data or trained models from a jupyter notebook. RasaClassifier \u00b6 The RasaClassifier takes a pretrained Rasa model and turns it into a scikit-learn compatible estimator. It expects text as input and it will predict an intent class. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict ([ \"hello there\" , \"are you a bot?\" ]) mod . predict_proba ([ \"hello there\" , \"are you a bot?\" ]) fetch_info_from_message ( self , text_input ) \u00b6 Show source code in scikit/classifier.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def fetch_info_from_message ( self , text_input ): \"\"\" Fetch all the info from a single text input. Can be used to also retreive entities. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.fetch_info_from_message(\"hello there\") ``` \"\"\" return self . interpreter . interpreter . parse ( text_input ) Fetch all the info from a single text input. Can be used to also retreive entities. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . fetch_info_from_message ( \"hello there\" ) predict ( self , X ) \u00b6 Show source code in scikit/classifier.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def predict ( self , X ): \"\"\" Makes a class prediction, scikit-style. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.predict([\"hello there\", \"are you a bot?\"]) ``` \"\"\" return np . array ([ self . fetch_info_from_message ( x )[ \"intent\" ][ \"name\" ] for x in X ]) Makes a class prediction, scikit-style. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict ([ \"hello there\" , \"are you a bot?\" ]) predict_proba ( self , X ) \u00b6 Show source code in scikit/classifier.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def predict_proba ( self , X ): \"\"\" Makes a class proba prediction, scikit-style. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.predict_proba([\"hello there\", \"are you a bot?\"]) ``` \"\"\" result = [] for x in X : ranking = self . fetch_info_from_message ( x )[ \"intent_ranking\" ] ranking_dict = { i [ \"name\" ]: i [ \"confidence\" ] for i in ranking } result . append ([ ranking_dict [ n ] for n in self . class_names_ ]) return np . array ( result ) Makes a class proba prediction, scikit-style. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict_proba ([ \"hello there\" , \"are you a bot?\" ]) nlu_path_to_dataframe ( path ) \u00b6 Show source code in scikit/common.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def nlu_path_to_dataframe ( path ): \"\"\" Converts a single nlu file with intents into a dataframe. Usage: ```python from rasa_nlu_examples.scikit import nlu_path_to_dataframe df = nlu_path_to_dataframe(\"path/to/nlu/nlu.yml\") ``` \"\"\" from rasa.nlu.convert import convert_training_data data = [] p = pathlib . Path ( path ) name = p . parts [ - 1 ] name = name [: name . find ( \".\" )] convert_training_data ( str ( p ), f \" { name } .json\" , output_format = \"json\" , language = \"en\" ) blob = json . loads ( pathlib . Path ( f \" { name } .json\" ) . read_text ()) for d in blob [ \"rasa_nlu_data\" ][ \"common_examples\" ]: data . append ({ \"text\" : d [ \"text\" ], \"label\" : d [ \"intent\" ]}) pathlib . Path ( f \" { name } .json\" ) . unlink () return pd . DataFrame ( data ) Converts a single nlu file with intents into a dataframe. Usage: from rasa_nlu_examples.scikit import nlu_path_to_dataframe df = nlu_path_to_dataframe ( \"path/to/nlu/nlu.yml\" )","title":"Overview"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.classifier.RasaClassifier","text":"The RasaClassifier takes a pretrained Rasa model and turns it into a scikit-learn compatible estimator. It expects text as input and it will predict an intent class. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict ([ \"hello there\" , \"are you a bot?\" ]) mod . predict_proba ([ \"hello there\" , \"are you a bot?\" ])","title":"RasaClassifier"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.classifier.RasaClassifier.fetch_info_from_message","text":"Show source code in scikit/classifier.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 def fetch_info_from_message ( self , text_input ): \"\"\" Fetch all the info from a single text input. Can be used to also retreive entities. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.fetch_info_from_message(\"hello there\") ``` \"\"\" return self . interpreter . interpreter . parse ( text_input ) Fetch all the info from a single text input. Can be used to also retreive entities. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . fetch_info_from_message ( \"hello there\" )","title":"fetch_info_from_message()"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.classifier.RasaClassifier.predict","text":"Show source code in scikit/classifier.py 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def predict ( self , X ): \"\"\" Makes a class prediction, scikit-style. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.predict([\"hello there\", \"are you a bot?\"]) ``` \"\"\" return np . array ([ self . fetch_info_from_message ( x )[ \"intent\" ][ \"name\" ] for x in X ]) Makes a class prediction, scikit-style. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict ([ \"hello there\" , \"are you a bot?\" ])","title":"predict()"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.classifier.RasaClassifier.predict_proba","text":"Show source code in scikit/classifier.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 def predict_proba ( self , X ): \"\"\" Makes a class proba prediction, scikit-style. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.predict_proba([\"hello there\", \"are you a bot?\"]) ``` \"\"\" result = [] for x in X : ranking = self . fetch_info_from_message ( x )[ \"intent_ranking\" ] ranking_dict = { i [ \"name\" ]: i [ \"confidence\" ] for i in ranking } result . append ([ ranking_dict [ n ] for n in self . class_names_ ]) return np . array ( result ) Makes a class proba prediction, scikit-style. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict_proba ([ \"hello there\" , \"are you a bot?\" ])","title":"predict_proba()"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.common.nlu_path_to_dataframe","text":"Show source code in scikit/common.py 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 def nlu_path_to_dataframe ( path ): \"\"\" Converts a single nlu file with intents into a dataframe. Usage: ```python from rasa_nlu_examples.scikit import nlu_path_to_dataframe df = nlu_path_to_dataframe(\"path/to/nlu/nlu.yml\") ``` \"\"\" from rasa.nlu.convert import convert_training_data data = [] p = pathlib . Path ( path ) name = p . parts [ - 1 ] name = name [: name . find ( \".\" )] convert_training_data ( str ( p ), f \" { name } .json\" , output_format = \"json\" , language = \"en\" ) blob = json . loads ( pathlib . Path ( f \" { name } .json\" ) . read_text ()) for d in blob [ \"rasa_nlu_data\" ][ \"common_examples\" ]: data . append ({ \"text\" : d [ \"text\" ], \"label\" : d [ \"intent\" ]}) pathlib . Path ( f \" { name } .json\" ) . unlink () return pd . DataFrame ( data ) Converts a single nlu file with intents into a dataframe. Usage: from rasa_nlu_examples.scikit import nlu_path_to_dataframe df = nlu_path_to_dataframe ( \"path/to/nlu/nlu.yml\" )","title":"nlu_path_to_dataframe()"},{"location":"docs/meta/printer/","text":"Here's an example configuration file that demonstrates how the custom printer component works. Configurable Variables \u00b6 alias : gives an extra name to the component and adds an extra message that is printed Base Usage \u00b6 When running this example you'll notice that what the effect is of the CountVectorsFeaturizer . You should see print statements appear when you talk to the assistant. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.meta.Printer alias : before count vectors - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.meta.Printer alias : after count vectors - name : DIETClassifier epochs : 100 When you now interact with your model via rasa shell you will see pretty information appear about the state of the Message object. It might look something like this; .r1 {font-weight: bold} .r2 {color: #008000} .r3 {color: #000080; font-weight: bold} { 'text' : 'rasa nlu examples' , 'intent' : { 'name' : 'out_of_scope' , 'confidence' : 0.4313829839229584 } , 'entities' : [ { 'entity' : 'proglang' , 'start' : 0 , 'end' : 4 , 'confidence_entity' : 0.42326217889785767 , 'value' : 'rasa' , 'extractor' : 'DIETClassifier' } ] , 'text_tokens' : [ 'rasa' , 'nlu' , 'examples' ] , 'intent_ranking' : [ { 'name' : 'out_of_scope' , 'confidence' : 0.4313829839229584 } , { 'name' : 'goodbye' , 'confidence' : 0.2445288747549057 } , { 'name' : 'bot_challenge' , 'confidence' : 0.23958507180213928 } , { 'name' : 'greet' , 'confidence' : 0.04896979033946991 } , { 'name' : 'talk_code' , 'confidence' : 0.035533301532268524 } ] , 'dense' : { 'sequence' : { 'shape' : ( 3 , 25 ) , 'dtype' : dtype ( 'float32' )} , 'sentence' : { 'shape' : ( 1 , 25 ) , 'dtype' : dtype ( 'float32' )} } , 'sparse' : { 'sequence' : { 'shape' : ( 3 , 1780 ) , 'dtype' : dtype ( 'float64' ) , 'stored_elements' : 67 } , 'sentence' : { 'shape' : ( 1 , 1756 ) , 'dtype' : dtype ( 'int64' ) , 'stored_elements' : 32 } } }","title":"Printer"},{"location":"docs/meta/printer/#configurable-variables","text":"alias : gives an extra name to the component and adds an extra message that is printed","title":"Configurable Variables"},{"location":"docs/meta/printer/#base-usage","text":"When running this example you'll notice that what the effect is of the CountVectorsFeaturizer . You should see print statements appear when you talk to the assistant. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.meta.Printer alias : before count vectors - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.meta.Printer alias : after count vectors - name : DIETClassifier epochs : 100 When you now interact with your model via rasa shell you will see pretty information appear about the state of the Message object. It might look something like this; .r1 {font-weight: bold} .r2 {color: #008000} .r3 {color: #000080; font-weight: bold} { 'text' : 'rasa nlu examples' , 'intent' : { 'name' : 'out_of_scope' , 'confidence' : 0.4313829839229584 } , 'entities' : [ { 'entity' : 'proglang' , 'start' : 0 , 'end' : 4 , 'confidence_entity' : 0.42326217889785767 , 'value' : 'rasa' , 'extractor' : 'DIETClassifier' } ] , 'text_tokens' : [ 'rasa' , 'nlu' , 'examples' ] , 'intent_ranking' : [ { 'name' : 'out_of_scope' , 'confidence' : 0.4313829839229584 } , { 'name' : 'goodbye' , 'confidence' : 0.2445288747549057 } , { 'name' : 'bot_challenge' , 'confidence' : 0.23958507180213928 } , { 'name' : 'greet' , 'confidence' : 0.04896979033946991 } , { 'name' : 'talk_code' , 'confidence' : 0.035533301532268524 } ] , 'dense' : { 'sequence' : { 'shape' : ( 3 , 25 ) , 'dtype' : dtype ( 'float32' )} , 'sentence' : { 'shape' : ( 1 , 25 ) , 'dtype' : dtype ( 'float32' )} } , 'sparse' : { 'sequence' : { 'shape' : ( 3 , 1780 ) , 'dtype' : dtype ( 'float64' ) , 'stored_elements' : 67 } , 'sentence' : { 'shape' : ( 1 , 1756 ) , 'dtype' : dtype ( 'int64' ) , 'stored_elements' : 32 } } }","title":"Base Usage"},{"location":"docs/tokenizer/stanza/","text":"The Stanza project from Stanford supports tokenizers, lemmatizers as well as part of speech detection for many languages that are not supported by spaCy. You can find the available languages here . Model Download \u00b6 To use a Stanza model you'll first need to download it. This can be done from python. import stanza # download English model in the ~/stanza_resources dir stanza . download ( 'en' , dir = '~/stanza_resources' ) Configurable Variables \u00b6 lang : then two-letter abbreprivation of the language you want to use cache_dir : pass it the name of the directory where you've downloaded/saved the embeddings Base Usage \u00b6 Once downloaded it can be used in a Rasa configuration, like below; language : en pipeline : - name : rasa_nlu_examples.tokenizers.StanzaTokenizer lang : \"en\" cache_dir : \"~/stanza_resources\" - name : LexicalSyntacticFeaturizer \"features\" : [ [ \"low\" , \"title\" , \"upper\" ], [ \"BOS\" , \"EOS\" , \"low\" , \"upper\" , \"title\" , \"digit\" , \"pos\" ], [ \"low\" , \"title\" , \"upper\" ], ] - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 One thing to note here is that the LexicalSyntacticFeaturizer will be able to pick up the \"pos\" information with the StanzaTokenizer just like you're able to do that with spaCy. The CountVectorsFeaturizer is able to pick up the lemma features that are generated.","title":"StanzaTokenizer"},{"location":"docs/tokenizer/stanza/#model-download","text":"To use a Stanza model you'll first need to download it. This can be done from python. import stanza # download English model in the ~/stanza_resources dir stanza . download ( 'en' , dir = '~/stanza_resources' )","title":"Model Download"},{"location":"docs/tokenizer/stanza/#configurable-variables","text":"lang : then two-letter abbreprivation of the language you want to use cache_dir : pass it the name of the directory where you've downloaded/saved the embeddings","title":"Configurable Variables"},{"location":"docs/tokenizer/stanza/#base-usage","text":"Once downloaded it can be used in a Rasa configuration, like below; language : en pipeline : - name : rasa_nlu_examples.tokenizers.StanzaTokenizer lang : \"en\" cache_dir : \"~/stanza_resources\" - name : LexicalSyntacticFeaturizer \"features\" : [ [ \"low\" , \"title\" , \"upper\" ], [ \"BOS\" , \"EOS\" , \"low\" , \"upper\" , \"title\" , \"digit\" , \"pos\" ], [ \"low\" , \"title\" , \"upper\" ], ] - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 One thing to note here is that the LexicalSyntacticFeaturizer will be able to pick up the \"pos\" information with the StanzaTokenizer just like you're able to do that with spaCy. The CountVectorsFeaturizer is able to pick up the lemma features that are generated.","title":"Base Usage"},{"location":"docs/tokenizer/thai_tokenizer/","text":"The ThaiTokenizer is a Rasa compatible tokenizer for Thai, using PyThaiNLP under the hood. In order to use the ThaiTokenizer the language must be set to th - no other languages are supported by this tokenizer. Configurable Variables \u00b6 None Base Usage \u00b6 The ThaiTokenizer can be used in a Rasa configuration like below: language : th pipeline : - name : rasa_nlu_examples.tokenizers.ThaiTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 If there are any issues with this tokenizer, please let us know .","title":"ThaiTokenizer"},{"location":"docs/tokenizer/thai_tokenizer/#configurable-variables","text":"None","title":"Configurable Variables"},{"location":"docs/tokenizer/thai_tokenizer/#base-usage","text":"The ThaiTokenizer can be used in a Rasa configuration like below: language : th pipeline : - name : rasa_nlu_examples.tokenizers.ThaiTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 If there are any issues with this tokenizer, please let us know .","title":"Base Usage"}]}