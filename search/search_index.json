{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rasa NLU Examples \u00b6 This repository contains some example components meant for educational and inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests and some documentation but this is a community project, not something that is part of core Rasa. The goal of these tools will be to be compatible with the most recent version of rasa only. You may need to point to an older release of the project if you want it to be compatible with an older version of Rasa. Compatibility \u00b6 This project currently supports components for Rasa 3.0. For older versions, see the list below. version 0.1.3 is the final release for Rasa 1.10 version 0.2.8 is the final release for Rasa 2.8 Tokenizers \u00b6 Tokenizers can split up the input text into tokens. Depending on the Tokenizer that you pick you can also choose to apply lemmatization. For languages that have rich grammatical features this might help reduce the size of all the possible tokens. rasa_nlu_examples.tokenizers.BlankSpacyTokenizer docs rasa_nlu_examples.tokenizers.ThaiTokenizer docs Featurizers \u00b6 Dense featurizers attach dense numeric features per token as well as to the entire utterance. These features are picked up by intent classifiers and entity detectors later in the pipeline. rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs rasa_nlu_examples.featurizers.dense.GensimFeaturizer docs rasa_nlu_examples.featurizers.sparse.TfIdfFeaturizer docs rasa_nlu_examples.featurizers.sparse.HashingFeaturizer docs Intent Classifiers \u00b6 Intent classifiers are models that predict an intent from a given user message text. The default intent classifier in Rasa NLU is the DIET model which can be fairly computationally expensive, especially if you do not need to detect entities. We provide some examples of alternative intent classifiers here. rasa_nlu_examples.classifiers.NaiveBayesClassifier docs rasa_nlu_examples.classifiers.LogisticRegressionClassifier docs Entity Extractors \u00b6 rasa_nlu_examples.extractor.FlashTextEntityExtractor docs rasa_nlu_examples.extractor.DateparserEntityExtractor docs Name Lists \u00b6 Language models are typically trained on Western datasets. That means that the reported benchmarks might not apply to your use-case. For example; detecting names in texts from France is not the same thing as detecting names in Madagascar. Even though French is used actively in both countries, the names of it's citizens might be so different that you cannot assume that the benchmarks apply universally. To remedy this we've started collecting name lists. These can be used as a lookup table which can be picked up by Rasa's RegexEntityExtractor or our FlashTextEntityExtractor . It won't be 100% perfect but it should give a reasonable starting point. You can find the namelists here . We currently offer namelists for the United States, Germany as well as common Arabic names. Feel free to submit PRs for more languages. We're also eager to receive feedback. Contributing \u00b6 You can find the contribution guide here .","title":"Home"},{"location":"#rasa-nlu-examples","text":"This repository contains some example components meant for educational and inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests and some documentation but this is a community project, not something that is part of core Rasa. The goal of these tools will be to be compatible with the most recent version of rasa only. You may need to point to an older release of the project if you want it to be compatible with an older version of Rasa.","title":"Rasa NLU Examples"},{"location":"#compatibility","text":"This project currently supports components for Rasa 3.0. For older versions, see the list below. version 0.1.3 is the final release for Rasa 1.10 version 0.2.8 is the final release for Rasa 2.8","title":"Compatibility"},{"location":"#tokenizers","text":"Tokenizers can split up the input text into tokens. Depending on the Tokenizer that you pick you can also choose to apply lemmatization. For languages that have rich grammatical features this might help reduce the size of all the possible tokens. rasa_nlu_examples.tokenizers.BlankSpacyTokenizer docs rasa_nlu_examples.tokenizers.ThaiTokenizer docs","title":"Tokenizers"},{"location":"#featurizers","text":"Dense featurizers attach dense numeric features per token as well as to the entire utterance. These features are picked up by intent classifiers and entity detectors later in the pipeline. rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs rasa_nlu_examples.featurizers.dense.GensimFeaturizer docs rasa_nlu_examples.featurizers.sparse.TfIdfFeaturizer docs rasa_nlu_examples.featurizers.sparse.HashingFeaturizer docs","title":"Featurizers"},{"location":"#intent-classifiers","text":"Intent classifiers are models that predict an intent from a given user message text. The default intent classifier in Rasa NLU is the DIET model which can be fairly computationally expensive, especially if you do not need to detect entities. We provide some examples of alternative intent classifiers here. rasa_nlu_examples.classifiers.NaiveBayesClassifier docs rasa_nlu_examples.classifiers.LogisticRegressionClassifier docs","title":"Intent Classifiers"},{"location":"#entity-extractors","text":"rasa_nlu_examples.extractor.FlashTextEntityExtractor docs rasa_nlu_examples.extractor.DateparserEntityExtractor docs","title":"Entity Extractors"},{"location":"#name-lists","text":"Language models are typically trained on Western datasets. That means that the reported benchmarks might not apply to your use-case. For example; detecting names in texts from France is not the same thing as detecting names in Madagascar. Even though French is used actively in both countries, the names of it's citizens might be so different that you cannot assume that the benchmarks apply universally. To remedy this we've started collecting name lists. These can be used as a lookup table which can be picked up by Rasa's RegexEntityExtractor or our FlashTextEntityExtractor . It won't be 100% perfect but it should give a reasonable starting point. You can find the namelists here . We currently offer namelists for the United States, Germany as well as common Arabic names. Feel free to submit PRs for more languages. We're also eager to receive feedback.","title":"Name Lists"},{"location":"#contributing","text":"You can find the contribution guide here .","title":"Contributing"},{"location":"benchmarking/","text":"Benchmarking Guide \u00b6 This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files with NLU components from this library. You can glance over some examples below. Basic Config Here's a very basic configuartion file. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: DIETClassifier epochs: 200 Assuming this file is named basic-config.yml you can run this pipeline as a benchmark by running this command from the project directory; rasa test nlu --config basic-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-config This will generate output in the gridresults/basic-config folder. Basic Byte-Pair Here's the same basic configuration but now with dense features added. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 1000 dim: 25 - name: DIETClassifier epochs: 200 Assuming this file is named basic-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config basic-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-bytepair-config This will generate output in the gridresults/basic-bytepair-config folder. Medium Byte-Pair We've now increased the vocabulary size and dimensionality. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 10000 dim: 100 - name: DIETClassifier epochs: 200 Assuming this file is named medium-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config medium-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/medium-bytepair-config This will generate output in the gridresults/medium-bytepair-config folder. Large Byte-Pair We've now grabbed the largest English Byte-Pair embeddings available. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 200000 dim: 300 - name: DIETClassifier epochs: 200 Assuming this file is named large-bytepair-config.yml you can run this benchmark by running this command from the project directory; rasa test nlu --config large-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/large-bytepair-config This will generate output in the gridresults/large-bytepair-config folder. Final Reminder \u00b6 We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline. The components that we host here may very well lag behind Rasa Open Source too.","title":"Benchmarking Guide"},{"location":"benchmarking/#benchmarking-guide","text":"This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files with NLU components from this library. You can glance over some examples below. Basic Config Here's a very basic configuartion file. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: DIETClassifier epochs: 200 Assuming this file is named basic-config.yml you can run this pipeline as a benchmark by running this command from the project directory; rasa test nlu --config basic-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-config This will generate output in the gridresults/basic-config folder. Basic Byte-Pair Here's the same basic configuration but now with dense features added. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 1000 dim: 25 - name: DIETClassifier epochs: 200 Assuming this file is named basic-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config basic-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-bytepair-config This will generate output in the gridresults/basic-bytepair-config folder. Medium Byte-Pair We've now increased the vocabulary size and dimensionality. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 10000 dim: 100 - name: DIETClassifier epochs: 200 Assuming this file is named medium-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config medium-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/medium-bytepair-config This will generate output in the gridresults/medium-bytepair-config folder. Large Byte-Pair We've now grabbed the largest English Byte-Pair embeddings available. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 200000 dim: 300 - name: DIETClassifier epochs: 200 Assuming this file is named large-bytepair-config.yml you can run this benchmark by running this command from the project directory; rasa test nlu --config large-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/large-bytepair-config This will generate output in the gridresults/large-bytepair-config folder.","title":"Benchmarking Guide"},{"location":"benchmarking/#final-reminder","text":"We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline. The components that we host here may very well lag behind Rasa Open Source too.","title":"Final Reminder"},{"location":"contributing/","text":"Contributing Guide \u00b6 Ways you can Contribute \u00b6 We're open to contributions and there are many ways that you can make one. You can suggest new features. You can help review new features. You can submit new components. You can let us know if there are bugs. You can let us know if the components in this library help you. Open an Issue \u00b6 You can submit an issue here . Issues allow us to keep track of a conversation about this repository and it is the preferred communication channel for bugs related to this project. Suggest a New Feature \u00b6 This project started because we wanted to offer support for word embeddings for more languages. The first feature we added was support for FastText, which offers embeddings for 157 languages. We later received a contribution from a community member for BytePair embeddings, which offers support for 275 languages. We weren't aware of these embeddings but we were exited to support more languages. Odds are that there are many more tools out there that the maintainers of this project aren't aware of yet. There may very well be more embeddings, tokenziers, lemmatizers and models that we're oblivious to but can help Rasa developers make better assistants. The goal of this project is to support more of these sorts of tools for Rasa users. You can help out the project just by letting us know if there's an integration missing! If you do not have the time to contribute a component yourself then you can still contribute to the effort by letting us know what components might help you make a better assistant. Share an Observation \u00b6 If the tools that we offer here turn out to be useful then we'd love to hear about it. We're also interested in hearing if these tools don't work for your usecase. Any feedback will be shared with the research team at Rasa. We're especially keen to hear feedback on the performance of the word embeddings for Non-English languages. You can leave a message either on the github issue list or on the Rasa forum . Adding a new Component \u00b6 There's a balance between allowing experimentation and maintaining all the code. This is why we've come up with a checklist that you should keep in mind before you're submitting code. If you want to contribute a new component please make an issue first so we can discuss it. We want to prevent double work where possible and make sure the proposed component is appropriate for this repository. New tools that are added here need to be plausibly useful in a real life scenario. For example, we won't accept a component that adds gaussian noise to the features. Think about unit tests. We prefer to standardise unit tests as much as possible but there may be specific things you'd like to check for. Check the guides! There's a great custom components guide on the Rasa documentation page. If you get stuck, you're free to start an issue on this repository or to start a thread on our forum . Removing a Component \u00b6 Just like components can be added, they can also be removed. Historically, we've removed components when we barely saw usage in the community or when the community felt that the tools were cumbersome to use and didnt contribute to the Rasa pipeline. The removal typically takes place whenever Rasa is updated and the new mayor version (from 1.x to 2.x , and from 2.x to 3.x ). Testing \u00b6 We run automated tests via GitHub actions but you can also run all the checking mechanisms locally. To run the tests locally you'll need to run some code beforehand. python tests/scripts/prepare_fasttext.py This will prepare the filesystem for testing. We do this to prevent the need of downloading very large word embeddings locally and in CI. Fasttext can be 6-7 GB and we don't want to pull such a payload at every CI step. You can also prepare files locally by installing all dependencies via the Makefile . make install You can also run all style and type checking mechanisms locally via the Makefile . make check Documentation \u00b6 We use mkdocs-material for our documentation. To generate the documentation locally you can run the following command: mkdocs serve If you're happy with the docs, you can push them to Github (assuming you've gotten the appropriate permissions) via: mkdocs gh-deploy This will push the build documentation to the docs branch, which is used for serving the documentation by GitHub pages.","title":"Contribute"},{"location":"contributing/#contributing-guide","text":"","title":"Contributing Guide"},{"location":"contributing/#ways-you-can-contribute","text":"We're open to contributions and there are many ways that you can make one. You can suggest new features. You can help review new features. You can submit new components. You can let us know if there are bugs. You can let us know if the components in this library help you.","title":"Ways you can Contribute"},{"location":"contributing/#open-an-issue","text":"You can submit an issue here . Issues allow us to keep track of a conversation about this repository and it is the preferred communication channel for bugs related to this project.","title":"Open an Issue"},{"location":"contributing/#suggest-a-new-feature","text":"This project started because we wanted to offer support for word embeddings for more languages. The first feature we added was support for FastText, which offers embeddings for 157 languages. We later received a contribution from a community member for BytePair embeddings, which offers support for 275 languages. We weren't aware of these embeddings but we were exited to support more languages. Odds are that there are many more tools out there that the maintainers of this project aren't aware of yet. There may very well be more embeddings, tokenziers, lemmatizers and models that we're oblivious to but can help Rasa developers make better assistants. The goal of this project is to support more of these sorts of tools for Rasa users. You can help out the project just by letting us know if there's an integration missing! If you do not have the time to contribute a component yourself then you can still contribute to the effort by letting us know what components might help you make a better assistant.","title":"Suggest a New Feature"},{"location":"contributing/#share-an-observation","text":"If the tools that we offer here turn out to be useful then we'd love to hear about it. We're also interested in hearing if these tools don't work for your usecase. Any feedback will be shared with the research team at Rasa. We're especially keen to hear feedback on the performance of the word embeddings for Non-English languages. You can leave a message either on the github issue list or on the Rasa forum .","title":"Share an Observation"},{"location":"contributing/#adding-a-new-component","text":"There's a balance between allowing experimentation and maintaining all the code. This is why we've come up with a checklist that you should keep in mind before you're submitting code. If you want to contribute a new component please make an issue first so we can discuss it. We want to prevent double work where possible and make sure the proposed component is appropriate for this repository. New tools that are added here need to be plausibly useful in a real life scenario. For example, we won't accept a component that adds gaussian noise to the features. Think about unit tests. We prefer to standardise unit tests as much as possible but there may be specific things you'd like to check for. Check the guides! There's a great custom components guide on the Rasa documentation page. If you get stuck, you're free to start an issue on this repository or to start a thread on our forum .","title":"Adding a new Component"},{"location":"contributing/#removing-a-component","text":"Just like components can be added, they can also be removed. Historically, we've removed components when we barely saw usage in the community or when the community felt that the tools were cumbersome to use and didnt contribute to the Rasa pipeline. The removal typically takes place whenever Rasa is updated and the new mayor version (from 1.x to 2.x , and from 2.x to 3.x ).","title":"Removing a Component"},{"location":"contributing/#testing","text":"We run automated tests via GitHub actions but you can also run all the checking mechanisms locally. To run the tests locally you'll need to run some code beforehand. python tests/scripts/prepare_fasttext.py This will prepare the filesystem for testing. We do this to prevent the need of downloading very large word embeddings locally and in CI. Fasttext can be 6-7 GB and we don't want to pull such a payload at every CI step. You can also prepare files locally by installing all dependencies via the Makefile . make install You can also run all style and type checking mechanisms locally via the Makefile . make check","title":"Testing"},{"location":"contributing/#documentation","text":"We use mkdocs-material for our documentation. To generate the documentation locally you can run the following command: mkdocs serve If you're happy with the docs, you can push them to Github (assuming you've gotten the appropriate permissions) via: mkdocs gh-deploy This will push the build documentation to the docs branch, which is used for serving the documentation by GitHub pages.","title":"Documentation"},{"location":"docs/classifier/logistic-regression/","text":"LogisticRegressionClassifier \u00b6 This intent classifier is based on the Logistic Regression Classifier from sklearn . This classifier only looks at sparse features extracted from the Rasa NLU feature pipeline and is a much faster alternative to neural models like DIET . Configurable Variables \u00b6 The classifier supports the same parameters as those that are listed in the sklearn documentation . The only difference is: there is no warm_start option the default class_weight is \"balanced\" Base Usage \u00b6 The configuration file below demonstrates how you might use the this component. In this example we are extracting sparse features with two CountVectorsFeaturizer instances, the first of which produces sparse bag-of-words features, and the second which produces sparse bags-of-character-ngram features. Note that in the following example, setting the class_weight parameter to None explicitly does have an effect because our default value for this paramter is \"balanced\". language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifier.LogisticRegressionClassifier class_weight : None Unlike DIET , this classifier only predicts intents. If you also need entity extraction, you will have to add a separate entity extractor to your config. Below is an example where we have included the CRFEntityExtractor to extract entities. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifier.LogisticRegressionClassifier - name : CRFEntityExtractor","title":"LogisticRegression"},{"location":"docs/classifier/logistic-regression/#logisticregressionclassifier","text":"This intent classifier is based on the Logistic Regression Classifier from sklearn . This classifier only looks at sparse features extracted from the Rasa NLU feature pipeline and is a much faster alternative to neural models like DIET .","title":"LogisticRegressionClassifier"},{"location":"docs/classifier/logistic-regression/#configurable-variables","text":"The classifier supports the same parameters as those that are listed in the sklearn documentation . The only difference is: there is no warm_start option the default class_weight is \"balanced\"","title":"Configurable Variables"},{"location":"docs/classifier/logistic-regression/#base-usage","text":"The configuration file below demonstrates how you might use the this component. In this example we are extracting sparse features with two CountVectorsFeaturizer instances, the first of which produces sparse bag-of-words features, and the second which produces sparse bags-of-character-ngram features. Note that in the following example, setting the class_weight parameter to None explicitly does have an effect because our default value for this paramter is \"balanced\". language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifier.LogisticRegressionClassifier class_weight : None Unlike DIET , this classifier only predicts intents. If you also need entity extraction, you will have to add a separate entity extractor to your config. Below is an example where we have included the CRFEntityExtractor to extract entities. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifier.LogisticRegressionClassifier - name : CRFEntityExtractor","title":"Base Usage"},{"location":"docs/classifier/naive-bayes/","text":"SparseNaiveBayesIntentClassifier \u00b6 This intent classifier is based on the Bernoulli-variant of the Na\u00efve Bayes classifier in sklearn . This classifier only looks at sparse features extracted from the Rasa NLU feature pipeline and is a faster alternative to neural models like DIET . This model requires that there be some sparse featurizers in your pipeleine. If you config only has dense features it will throw an exception. Configurable Variables \u00b6 alpha (default: 1.0): Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). binarize (default: 0.0): Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors. fit_prior (default: True): Whether to learn class prior probabilities or not. If false, a uniform prior will be used. class_prior (default: None): Prior probabilities (as a list) of the classes. If specified the priors are not adjusted according to the data. Base Usage \u00b6 The configuration file below demonstrates how you might use the this component. In this example we are extracting sparse features with two CountVectorsFeaturizer instances, the first of which produces sparse bag-of-words features, and the second which produces sparse bags-of-character-ngram features. We've also set the alpha smoothing parameter to 0.1. language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifier.SparseNaiveBayesClassifier alpha : 0.1 Unlike DIET , this classifier only predicts intents. If you also need entity extraction, you will have to add a separate entity extractor to your config. Below is an example where we have included the CRFEntityExtractor to extract entities. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifier.SparseNaiveBayesClassifier alpha : 0.1 - name : CRFEntityExtractor","title":"SparseNaiveBayes"},{"location":"docs/classifier/naive-bayes/#sparsenaivebayesintentclassifier","text":"This intent classifier is based on the Bernoulli-variant of the Na\u00efve Bayes classifier in sklearn . This classifier only looks at sparse features extracted from the Rasa NLU feature pipeline and is a faster alternative to neural models like DIET . This model requires that there be some sparse featurizers in your pipeleine. If you config only has dense features it will throw an exception.","title":"SparseNaiveBayesIntentClassifier"},{"location":"docs/classifier/naive-bayes/#configurable-variables","text":"alpha (default: 1.0): Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). binarize (default: 0.0): Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors. fit_prior (default: True): Whether to learn class prior probabilities or not. If false, a uniform prior will be used. class_prior (default: None): Prior probabilities (as a list) of the classes. If specified the priors are not adjusted according to the data.","title":"Configurable Variables"},{"location":"docs/classifier/naive-bayes/#base-usage","text":"The configuration file below demonstrates how you might use the this component. In this example we are extracting sparse features with two CountVectorsFeaturizer instances, the first of which produces sparse bag-of-words features, and the second which produces sparse bags-of-character-ngram features. We've also set the alpha smoothing parameter to 0.1. language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifier.SparseNaiveBayesClassifier alpha : 0.1 Unlike DIET , this classifier only predicts intents. If you also need entity extraction, you will have to add a separate entity extractor to your config. Below is an example where we have included the CRFEntityExtractor to extract entities. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifier.SparseNaiveBayesClassifier alpha : 0.1 - name : CRFEntityExtractor","title":"Base Usage"},{"location":"docs/extractors/dateparser/","text":"DateparserEntityExtractor \u00b6 Note If you want to use this component, be sure to either install flashtext manually or use our convenience installer. python -m pip install \"rasa_nlu_examples[dateparser] @ git+https://github.com/RasaHQ/rasa-nlu-examples.git\" What does it do? \u00b6 This entity extractor uses the dateparser to extract entities that resemble dates. You can get a demo by running the code below. from rasa.shared.nlu.training_data.message import Message from rasa_nlu_examples.extractors.dateparser_extractor import DateparserEntityExtractor from rich import print msg = Message . build ( \"hello tomorrow, goodbye yesterday\" ,) extractor = DateparserEntityExtractor ({}) extractor . process ( msg ) print ( msg . as_dict_nlu ()) This will parse the following information. { 'text' : 'hello tomorrow, goodbye yesterday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 6 , 'end' : 14 , 'value' : 'tomorrow' , 'parsed_date' : '2021-06-05 11:50:10.502082' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' }, { 'entity' : 'DATETIME_REFERENCE' , 'start' : 24 , 'end' : 33 , 'value' : 'yesterday' , 'parsed_date' : '2021-06-03 11:50:10.503160' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } Note that we add an extra parsed_date key to the entity dictionary here. Another benefit of dateparser is that it also contains rules for Non-English languages. Here is a Dutch example. { 'text' : 'ik wil een pizza bestellen voor morgen' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 32 , 'end' : 38 , 'value' : 'morgen' , 'parsed_date' : '2021-06-05 11:50:10.708588' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } It's also possible to configure the DateparserEntityExtractor to prefer dates in the future or in the past. That way, if somebody talks about Thursday can be picked up as next Thursday, allowing us to still parse out a date. \"Future\" Results \u00b6 This ran on Friday the 4th of June, 2021. { 'text' : 'i want a pizza thursday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 15 , 'end' : 23 , 'value' : 'thursday' , 'parsed_date' : '2021-06-10 00:00:00' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } \"Past\" Results \u00b6 This ran on Friday the 4th of June, 2021. { 'text' : 'i want to buy a pizza thursday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 22 , 'end' : 30 , 'value' : 'thursday' , 'parsed_date' : '2021-06-03 00:00:00' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } Configurable Variables \u00b6 languages : pass a list of languages that you want the parser to focus on, can be None but this setting is likely to overfit on English assumptions prefer_dates_from : can be either \"future\", \"past\" or None relative_base : can be a datestring that represents a reference date, this is useful when a user mentions \"tomorrow\", default None points to todays date Base Usage \u00b6 The configuration below is an example of how you might use FlashTextEntityExtractor . language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 - name : rasa_nlu_examples.extractors.DateparserEntityExtractor languages : [ \"en\" , \"nl\" , \"es\" ] prefer_dates_from : \"future\" Note that this entity extractor completely ignores the tokeniser. There might also be overlap with enities from other engines, like DIET and spaCy. Relative Base Usage \u00b6 language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 - name : rasa_nlu_examples.extractors.DateparserEntityExtractor languages : [ \"en\" , \"nl\" , \"es\" ] prefer_dates_from : \"future\" relative_base : \"2020-01-01\"","title":"DateParser"},{"location":"docs/extractors/dateparser/#dateparserentityextractor","text":"Note If you want to use this component, be sure to either install flashtext manually or use our convenience installer. python -m pip install \"rasa_nlu_examples[dateparser] @ git+https://github.com/RasaHQ/rasa-nlu-examples.git\"","title":"DateparserEntityExtractor"},{"location":"docs/extractors/dateparser/#what-does-it-do","text":"This entity extractor uses the dateparser to extract entities that resemble dates. You can get a demo by running the code below. from rasa.shared.nlu.training_data.message import Message from rasa_nlu_examples.extractors.dateparser_extractor import DateparserEntityExtractor from rich import print msg = Message . build ( \"hello tomorrow, goodbye yesterday\" ,) extractor = DateparserEntityExtractor ({}) extractor . process ( msg ) print ( msg . as_dict_nlu ()) This will parse the following information. { 'text' : 'hello tomorrow, goodbye yesterday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 6 , 'end' : 14 , 'value' : 'tomorrow' , 'parsed_date' : '2021-06-05 11:50:10.502082' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' }, { 'entity' : 'DATETIME_REFERENCE' , 'start' : 24 , 'end' : 33 , 'value' : 'yesterday' , 'parsed_date' : '2021-06-03 11:50:10.503160' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } Note that we add an extra parsed_date key to the entity dictionary here. Another benefit of dateparser is that it also contains rules for Non-English languages. Here is a Dutch example. { 'text' : 'ik wil een pizza bestellen voor morgen' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 32 , 'end' : 38 , 'value' : 'morgen' , 'parsed_date' : '2021-06-05 11:50:10.708588' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } It's also possible to configure the DateparserEntityExtractor to prefer dates in the future or in the past. That way, if somebody talks about Thursday can be picked up as next Thursday, allowing us to still parse out a date.","title":"What does it do?"},{"location":"docs/extractors/dateparser/#future-results","text":"This ran on Friday the 4th of June, 2021. { 'text' : 'i want a pizza thursday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 15 , 'end' : 23 , 'value' : 'thursday' , 'parsed_date' : '2021-06-10 00:00:00' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] }","title":"\"Future\" Results"},{"location":"docs/extractors/dateparser/#past-results","text":"This ran on Friday the 4th of June, 2021. { 'text' : 'i want to buy a pizza thursday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 22 , 'end' : 30 , 'value' : 'thursday' , 'parsed_date' : '2021-06-03 00:00:00' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] }","title":"\"Past\" Results"},{"location":"docs/extractors/dateparser/#configurable-variables","text":"languages : pass a list of languages that you want the parser to focus on, can be None but this setting is likely to overfit on English assumptions prefer_dates_from : can be either \"future\", \"past\" or None relative_base : can be a datestring that represents a reference date, this is useful when a user mentions \"tomorrow\", default None points to todays date","title":"Configurable Variables"},{"location":"docs/extractors/dateparser/#base-usage","text":"The configuration below is an example of how you might use FlashTextEntityExtractor . language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 - name : rasa_nlu_examples.extractors.DateparserEntityExtractor languages : [ \"en\" , \"nl\" , \"es\" ] prefer_dates_from : \"future\" Note that this entity extractor completely ignores the tokeniser. There might also be overlap with enities from other engines, like DIET and spaCy.","title":"Base Usage"},{"location":"docs/extractors/dateparser/#relative-base-usage","text":"language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 - name : rasa_nlu_examples.extractors.DateparserEntityExtractor languages : [ \"en\" , \"nl\" , \"es\" ] prefer_dates_from : \"future\" relative_base : \"2020-01-01\"","title":"Relative Base Usage"},{"location":"docs/extractors/flashtext/","text":"FlashTextEntityExtractor \u00b6 Note If you want to use this component, be sure to either install flashtext manually or use our convenience installer. python -m pip install \"rasa_nlu_examples[flashtext] @ git+https://github.com/RasaHQ/rasa-nlu-examples.git\" This entity extractor uses the flashtext library to extract entities using lookup tables . This is similar to RegexEntityExtractor , but different in a few ways: FlashTextEntityExtractor takes only lookups , not regex patterns FlashTextEntityExtractor matches using whitespace word boundaries. You cannot set it to match words regardless of boundaries. FlashTextEntityExtractor is much faster than RegexEntityExtractor . This is especially true for large lookup tables. Also note that anything other than [A-Za-z0-9_] is considered a word boundary. To add more non-word boundaries use the parameter non_word_boundaries Configurable Variables \u00b6 case_sensitive : whether to consider case when matching entities. False by default. non_word_boundaries : characters which shouldn't be considered word boundaries. Base Usage \u00b6 The configuration below is an example of how you might use FlashTextEntityExtractor . language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.extractors.FlashTextEntityExtractor case_sensitive : True non_word_boundary : - \"_\" - \",\" - name : DIETClassifier epochs : 100 You must include lookup tables in your NLU data. This might look like: nlu : - lookup : country examples : | - Afghanistan - Albania - ... - Zambia - Zimbabwe In this example, anytime a user's utterance contains an exact match for a country from the lookup table above, FlashTextEntityExtractor will extract this as an entity with type country . You should include a few examples with this entity in your intent data, like so: - intent : inform_home_country examples : | - I am from [Afghanistan](country) - My family is from [Albania](country","title":"FlashText"},{"location":"docs/extractors/flashtext/#flashtextentityextractor","text":"Note If you want to use this component, be sure to either install flashtext manually or use our convenience installer. python -m pip install \"rasa_nlu_examples[flashtext] @ git+https://github.com/RasaHQ/rasa-nlu-examples.git\" This entity extractor uses the flashtext library to extract entities using lookup tables . This is similar to RegexEntityExtractor , but different in a few ways: FlashTextEntityExtractor takes only lookups , not regex patterns FlashTextEntityExtractor matches using whitespace word boundaries. You cannot set it to match words regardless of boundaries. FlashTextEntityExtractor is much faster than RegexEntityExtractor . This is especially true for large lookup tables. Also note that anything other than [A-Za-z0-9_] is considered a word boundary. To add more non-word boundaries use the parameter non_word_boundaries","title":"FlashTextEntityExtractor"},{"location":"docs/extractors/flashtext/#configurable-variables","text":"case_sensitive : whether to consider case when matching entities. False by default. non_word_boundaries : characters which shouldn't be considered word boundaries.","title":"Configurable Variables"},{"location":"docs/extractors/flashtext/#base-usage","text":"The configuration below is an example of how you might use FlashTextEntityExtractor . language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.extractors.FlashTextEntityExtractor case_sensitive : True non_word_boundary : - \"_\" - \",\" - name : DIETClassifier epochs : 100 You must include lookup tables in your NLU data. This might look like: nlu : - lookup : country examples : | - Afghanistan - Albania - ... - Zambia - Zimbabwe In this example, anytime a user's utterance contains an exact match for a country from the lookup table above, FlashTextEntityExtractor will extract this as an entity with type country . You should include a few examples with this entity in your intent data, like so: - intent : inform_home_country examples : | - I am from [Afghanistan](country) - My family is from [Albania](country","title":"Base Usage"},{"location":"docs/featurizer/bytepair/","text":"This featurizer is a dense featurizer. If you're interested in learning how these work you might appreciate reading the original article . Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. A main feature of these types of embeddings is that they are relatively lightweight but also that they're availability in many languages. BytePair embeddings exist for 277 languages that are pretrained on wikipedia. More information on these embeddings can be found here . When you scroll down you will notice a large of languages that are available. Here's some examples from that list that give a detailed view of available vectors: Abkhazian Zulu English Hindi Chinese Esperanto Multi Language Configurable Variables \u00b6 lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25 , vs : specifies the vocabulary size of the segmentation model, default = 1000 , vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default= True Base Usage \u00b6 The configuration file below demonstrates how you might use the BytePair embeddings. The component will automatically download the required embeddings and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100","title":"BytePairFeaturizer"},{"location":"docs/featurizer/bytepair/#configurable-variables","text":"lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25 , vs : specifies the vocabulary size of the segmentation model, default = 1000 , vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default= True","title":"Configurable Variables"},{"location":"docs/featurizer/bytepair/#base-usage","text":"The configuration file below demonstrates how you might use the BytePair embeddings. The component will automatically download the required embeddings and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/fasttext/","text":"Fasttext supports word embeddings for 157 languages and is trained on both Common Crawl and Wikipedia. You can download the embeddings here . Note that this featurizer is a dense featurizer. Beware that these embedding files tend to be big: about 6-7Gb. It may be a better idea to train your own fasttext embeddings on your own data to save on disk space. Note In order to use this tool you'll need to ensure the correct dependencies are installed. pip install \"rasa_nlu_examples[fasttext] @ https://github.com/RasaHQ/rasa-nlu-examples.git\" Configurable Variables \u00b6 cache_path : pass it the name of the filepath where you've downloaded/saved the embeddings Base Usage \u00b6 The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_path : path/to/cc.nl.300.bin - name : DIETClassifier epochs : 100","title":"FastTextFeaturizer"},{"location":"docs/featurizer/fasttext/#configurable-variables","text":"cache_path : pass it the name of the filepath where you've downloaded/saved the embeddings","title":"Configurable Variables"},{"location":"docs/featurizer/fasttext/#base-usage","text":"The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_path : path/to/cc.nl.300.bin - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/gensim/","text":"This page discusses some properties of the GensimFeaturizer . Note that this featurizer is a dense featurizer. Gensim is a popular python library that makes it relatively easy to train your own word vectors. This can be useful if your corpus is very different than what most popular embeddings are trained on. We'll give a small guide on how to train your own embeddings here but you can also read the guide on the gensim docs . Training Your Own \u00b6 Training your own gensim model can be done in a few lines of code. A demonstration is shown below. from gensim.models import Word2Vec # Gensim needs a list of lists to represent tokens in a document. # In real life you\u2019d read a text file and turn it into lists here. text = [ \"this is a sentence\" , \"so is this\" , \"and we're all talking\" ] tokens = [ t . split ( \" \" ) for t in text ] # This is where we train new word embeddings. model = Word2Vec ( sentences = tokens , size = 10 , window = 3 , min_count = 1 , iter = 5 , workers = 2 ) # This is where they are saved to disk. model . wv . save ( \"wordvectors.kv\" ) This wordvectors.kv file should contain all the vectors that you've trained. It's this file that you can pass on to this component. Configurable Variables \u00b6 cache_path : pass it the name of the filepath where you've downloaded/saved the embeddings Base Usage \u00b6 The configuration file below demonstrates how you might use the gensim embeddings. In this example we're building a pipeline for the English language and we're assuming that you've trained your own embeddings which have been saved upfront as saved/beforehand/filename.kv . language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.GensimFeaturizer cache_path : path/to/filename.kv - name : DIETClassifier epochs : 100","title":"GensimFeaturizer"},{"location":"docs/featurizer/gensim/#training-your-own","text":"Training your own gensim model can be done in a few lines of code. A demonstration is shown below. from gensim.models import Word2Vec # Gensim needs a list of lists to represent tokens in a document. # In real life you\u2019d read a text file and turn it into lists here. text = [ \"this is a sentence\" , \"so is this\" , \"and we're all talking\" ] tokens = [ t . split ( \" \" ) for t in text ] # This is where we train new word embeddings. model = Word2Vec ( sentences = tokens , size = 10 , window = 3 , min_count = 1 , iter = 5 , workers = 2 ) # This is where they are saved to disk. model . wv . save ( \"wordvectors.kv\" ) This wordvectors.kv file should contain all the vectors that you've trained. It's this file that you can pass on to this component.","title":"Training Your Own"},{"location":"docs/featurizer/gensim/#configurable-variables","text":"cache_path : pass it the name of the filepath where you've downloaded/saved the embeddings","title":"Configurable Variables"},{"location":"docs/featurizer/gensim/#base-usage","text":"The configuration file below demonstrates how you might use the gensim embeddings. In this example we're building a pipeline for the English language and we're assuming that you've trained your own embeddings which have been saved upfront as saved/beforehand/filename.kv . language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.GensimFeaturizer cache_path : path/to/filename.kv - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/hashing/","text":"This page discusses some properties of the HashingFeaturizer . Note that this featurizer is a sparse featurizer. The featurizer is a wrapper a around scikit-learn's HashingVectorizer . It uses the \"hashing trick\" to transform input texts to a sparse vector by mapping each token to a column index using a fixed hash function. The featurizer has no state and cannot be trained. For a small number of columns (defined by the n_features parameter), hash coalitions are more likely, meaning that two words can get mapped to the same index. Configurable Variables \u00b6 n_features : the number of columns the input is mapped to. analyzer : determines how tokens are split. possible choices are word , char and char_wb . lowercase : convert input strings to lowercase. strip_accents : remove accents using one of the methods ascii or unicode . stop_words : filter by a list of stop words. min_ngram : the lower boundary of the range of n-values for different word n-grams or char n-grams to be extracted. max_ngram : the upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. norm : the normalization applied to each row vector (options are l1 , l2 or null ). binary : if True , all non-zero elements are mapped to 1 , instead of absolute counts. alternate_sign : apply the sign of the hashing function in order to reduce the effect of hash coalitions. Base Usage \u00b6 The configuration file below demonstrates how you might use the hashing featurizer. pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1024 - name : DIETClassifier epochs : 100 Combining several hashing featurizers \u00b6 In order to use a combination of several hash functions, multiple HashingFeaturizer instances can be added to the pipeline. However, note that since the hash function is deterministic, one needs to set a varying number of n_features for each. Otherwise one would end up with the same sparse vector being concatenated multiple times. See the discussion here how this, in combination with DIETClassifier , is related to Bloom embeddings . pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1021 - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1022 - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1023 - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1024 - name : DIETClassifier epochs : 100","title":"HashingFeaturizer"},{"location":"docs/featurizer/hashing/#configurable-variables","text":"n_features : the number of columns the input is mapped to. analyzer : determines how tokens are split. possible choices are word , char and char_wb . lowercase : convert input strings to lowercase. strip_accents : remove accents using one of the methods ascii or unicode . stop_words : filter by a list of stop words. min_ngram : the lower boundary of the range of n-values for different word n-grams or char n-grams to be extracted. max_ngram : the upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. norm : the normalization applied to each row vector (options are l1 , l2 or null ). binary : if True , all non-zero elements are mapped to 1 , instead of absolute counts. alternate_sign : apply the sign of the hashing function in order to reduce the effect of hash coalitions.","title":"Configurable Variables"},{"location":"docs/featurizer/hashing/#base-usage","text":"The configuration file below demonstrates how you might use the hashing featurizer. pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1024 - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/hashing/#combining-several-hashing-featurizers","text":"In order to use a combination of several hash functions, multiple HashingFeaturizer instances can be added to the pipeline. However, note that since the hash function is deterministic, one needs to set a varying number of n_features for each. Otherwise one would end up with the same sparse vector being concatenated multiple times. See the discussion here how this, in combination with DIETClassifier , is related to Bloom embeddings . pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1021 - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1022 - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1023 - name : rasa_nlu_examples.featurizers.sparse.HashingFeaturizer n_features : 1024 - name : DIETClassifier epochs : 100","title":"Combining several hashing featurizers"},{"location":"docs/featurizer/tfidf/","text":"This featurizer is a sparse featurizer. It builds on the scikit-learn implementation to convert text into sparse features that take the frequency of words into account. If we were to feed the direct count data directly to a classifier very frequent terms might shadow the frequencies of rarer, but potentially more interesting words. Configurable Variables \u00b6 analyzer : determines how tokens are split. possible choices are word , char and char_wb , default is word . min_ngram : the lower boundary of the range of n-values for different word n-grams or char n-grams to be extracted. max_ngram : the upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted. Base Usage \u00b6 The configuration file below demonstrates how you might use the TfIdfFeaturizer featurizer. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.sparse.TfIdfFeaturizer min_ngram : 1 max_ngram : 2 - name : DIETClassifier epochs : 100","title":"TfIdfFeaturizer"},{"location":"docs/featurizer/tfidf/#configurable-variables","text":"analyzer : determines how tokens are split. possible choices are word , char and char_wb , default is word . min_ngram : the lower boundary of the range of n-values for different word n-grams or char n-grams to be extracted. max_ngram : the upper boundary of the range of n-values for different word n-grams or char n-grams to be extracted.","title":"Configurable Variables"},{"location":"docs/featurizer/tfidf/#base-usage","text":"The configuration file below demonstrates how you might use the TfIdfFeaturizer featurizer. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.sparse.TfIdfFeaturizer min_ngram : 1 max_ngram : 2 - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/tokenizer/spacy-tokenizer/","text":"Rasa natively supports spaCy models that have a language model attached. But spaCy also offers tokenizers without a model. We support these tokenisers with this component. Note In order to use this tool you'll need to ensure that spaCy is installed with Rasa. pip install rasa[spacy] You should also be aware that for certain languages extra dependencies are required. More information is given on the spacy documentation . Configurable Variables \u00b6 lang : the two-letter abbreviation of the language you want to use. Base Usage \u00b6 Once downloaded it can be used in a Rasa configuration, like below; language : en pipeline : - name : rasa_nlu_examples.tokenizers.BlankSpacyTokenizer lang : \"en\" - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100","title":"BlankSpacyTokenizer"},{"location":"docs/tokenizer/spacy-tokenizer/#configurable-variables","text":"lang : the two-letter abbreviation of the language you want to use.","title":"Configurable Variables"},{"location":"docs/tokenizer/spacy-tokenizer/#base-usage","text":"Once downloaded it can be used in a Rasa configuration, like below; language : en pipeline : - name : rasa_nlu_examples.tokenizers.BlankSpacyTokenizer lang : \"en\" - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/tokenizer/thai-tokenizer/","text":"The ThaiTokenizer is a Rasa compatible tokenizer for Thai, using PyThaiNLP under the hood. In order to use the ThaiTokenizer the language must be set to th - no other languages are supported by this tokenizer. Note In order to use this tool you'll need to ensure the correct dependencies are installed. pip install \"rasa_nlu_examples[thai] @ https://github.com/RasaHQ/rasa-nlu-examples.git\" Configurable Variables \u00b6 None Base Usage \u00b6 The ThaiTokenizer can be used in a Rasa configuration like below: language : th pipeline : - name : rasa_nlu_examples.tokenizers.ThaiTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 If there are any issues with this tokenizer, please let us know .","title":"ThaiTokenizer"},{"location":"docs/tokenizer/thai-tokenizer/#configurable-variables","text":"None","title":"Configurable Variables"},{"location":"docs/tokenizer/thai-tokenizer/#base-usage","text":"The ThaiTokenizer can be used in a Rasa configuration like below: language : th pipeline : - name : rasa_nlu_examples.tokenizers.ThaiTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 If there are any issues with this tokenizer, please let us know .","title":"Base Usage"}]}