{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rasa NLU Examples \u00b6 This repository contains some example components meant for educational and inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests and some documentation but this is a community project, not something that is part of core Rasa. Components \u00b6 The goal of these tools will be to be compatible with the most recent version of rasa only. You may need to point to an older release of the project if you want it to be compatible with an older version of Rasa. The following components are implemented. Meta \u00b6 Printer \u00b6 rasa_nlu_examples.meta.Printer docs This component will print what each featurizer adds to the NLU message. Very useful for debugging. You can find a tutorial on it here . Dense Featurizers \u00b6 FastTextFeaturizer \u00b6 rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs These are the pretrained embeddings from FastText, see for more info here . These are available in 157 languages, see here . BytePairFeaturizer \u00b6 rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs These BytePair embeddings are specialized subword embeddings that are built to be lightweight. See this link for more information. These are available in 227 languages and you can specify the subword vocabulary size as well as the dimensionality. Contributing \u00b6 You can find the contribution guide here .","title":"Home"},{"location":"#rasa-nlu-examples","text":"This repository contains some example components meant for educational and inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests and some documentation but this is a community project, not something that is part of core Rasa.","title":"Rasa NLU Examples"},{"location":"#components","text":"The goal of these tools will be to be compatible with the most recent version of rasa only. You may need to point to an older release of the project if you want it to be compatible with an older version of Rasa. The following components are implemented.","title":"Components"},{"location":"#meta","text":"","title":"Meta"},{"location":"#printer","text":"rasa_nlu_examples.meta.Printer docs This component will print what each featurizer adds to the NLU message. Very useful for debugging. You can find a tutorial on it here .","title":"Printer"},{"location":"#dense-featurizers","text":"","title":"Dense Featurizers"},{"location":"#fasttextfeaturizer","text":"rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs These are the pretrained embeddings from FastText, see for more info here . These are available in 157 languages, see here .","title":"FastTextFeaturizer"},{"location":"#bytepairfeaturizer","text":"rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs These BytePair embeddings are specialized subword embeddings that are built to be lightweight. See this link for more information. These are available in 227 languages and you can specify the subword vocabulary size as well as the dimensionality.","title":"BytePairFeaturizer"},{"location":"#contributing","text":"You can find the contribution guide here .","title":"Contributing"},{"location":"benchmarking/","text":"Benchmarking Guide \u00b6 This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files like this one; Basic Config Here's a very basic configuartion file. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: DIETClassifier epochs: 200 Assuming this file is named basic-config.yml you can run this pipeline as a benchmark by running this command from the project directory; rasa test nlu --config basic-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-config This will generate output in the gridresults/basic-config folder. Basic Byte-Pair Here's the same basic configuration but now with dense features added. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 1000 dim: 25 - name: DIETClassifier epochs: 200 Assuming this file is named basic-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config basic-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-bytepair-config This will generate output in the gridresults/basic-bytepair-config folder. Medium Byte-Pair We've now increased the vocabulary size and dimensionality. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 10000 dim: 100 - name: DIETClassifier epochs: 200 Assuming this file is named medium-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config medium-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/medium-bytepair-config This will generate output in the gridresults/medium-bytepair-config folder. Large Byte-Pair We've now grabbed the largest English Byte-Pair embeddings available. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 200000 dim: 300 - name: DIETClassifier epochs: 200 Assuming this file is named large-bytepair-config.yml you can run this benchmark by running this command from the project directory; rasa test nlu --config large-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/large-bytepair-config This will generate output in the gridresults/large-bytepair-config folder. Final Reminder \u00b6 There's one thing to remind ourselves of at this phase in time. We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline and the components that we host here may very well lag behind a version of Rasa.","title":"Benchmarking Guide"},{"location":"benchmarking/#benchmarking-guide","text":"This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files like this one; Basic Config Here's a very basic configuartion file. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: DIETClassifier epochs: 200 Assuming this file is named basic-config.yml you can run this pipeline as a benchmark by running this command from the project directory; rasa test nlu --config basic-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-config This will generate output in the gridresults/basic-config folder. Basic Byte-Pair Here's the same basic configuration but now with dense features added. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 1000 dim: 25 - name: DIETClassifier epochs: 200 Assuming this file is named basic-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config basic-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-bytepair-config This will generate output in the gridresults/basic-bytepair-config folder. Medium Byte-Pair We've now increased the vocabulary size and dimensionality. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 10000 dim: 100 - name: DIETClassifier epochs: 200 Assuming this file is named medium-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config medium-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/medium-bytepair-config This will generate output in the gridresults/medium-bytepair-config folder. Large Byte-Pair We've now grabbed the largest English Byte-Pair embeddings available. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt token_pattern: (?u)\\b\\w+\\b - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 200000 dim: 300 - name: DIETClassifier epochs: 200 Assuming this file is named large-bytepair-config.yml you can run this benchmark by running this command from the project directory; rasa test nlu --config large-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/large-bytepair-config This will generate output in the gridresults/large-bytepair-config folder.","title":"Benchmarking Guide"},{"location":"benchmarking/#final-reminder","text":"There's one thing to remind ourselves of at this phase in time. We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline and the components that we host here may very well lag behind a version of Rasa.","title":"Final Reminder"},{"location":"contributing/","text":"Contributing Guide \u00b6 Ways you can Contribute \u00b6 We're open to contributions and there are many ways that you can make a contribution. Open an Issue \u00b6 Did you find a bug? Please let us know! You can submit an issue here . Share an Observation \u00b6 If the tools that we offer here turn out to be useful then we'd love to hear about it. The research team will consider all feedback and we're especially keen to hear feedback from non-English languages that try out some of the new embeddings. You can leave a message either on the github issue list or on the Rasa forum . Be sure to ping koaning on the forum if you mention this project, he's the maintainer. Adding a new Component \u00b6 There's a balance between allowing experimentation and maintaining all the code. So we've come up with the following checklist. If you want to contribute a new component please make an issue first so we can discuss it. We want to prevent double work where possible and make sure it is appropriate. New tools that are added here need to be plausibly useful. For example, we won't be able to accept a component that adds noise to the features. Think about unit tests. We prefer to standardise unit tests as much as possible but there may be specific things you'd like to check for. Testing \u00b6 To run the tests locally you'll need to run a script beforehand. python tests/prepare_everything.py This will prepare the filesystem for testing. We do this to prevent the need of downloading very large word embeddings locally and in CI. Fasttext can be 6-7 GB and we don't want to pull such a payload at every CI step. You can also prepare files locally by installing all dependencies via the Makefile . make install You can also run all style and type checking mechanisms locally via the Makefile . make check","title":"Contributing"},{"location":"contributing/#contributing-guide","text":"","title":"Contributing Guide"},{"location":"contributing/#ways-you-can-contribute","text":"We're open to contributions and there are many ways that you can make a contribution.","title":"Ways you can Contribute"},{"location":"contributing/#open-an-issue","text":"Did you find a bug? Please let us know! You can submit an issue here .","title":"Open an Issue"},{"location":"contributing/#share-an-observation","text":"If the tools that we offer here turn out to be useful then we'd love to hear about it. The research team will consider all feedback and we're especially keen to hear feedback from non-English languages that try out some of the new embeddings. You can leave a message either on the github issue list or on the Rasa forum . Be sure to ping koaning on the forum if you mention this project, he's the maintainer.","title":"Share an Observation"},{"location":"contributing/#adding-a-new-component","text":"There's a balance between allowing experimentation and maintaining all the code. So we've come up with the following checklist. If you want to contribute a new component please make an issue first so we can discuss it. We want to prevent double work where possible and make sure it is appropriate. New tools that are added here need to be plausibly useful. For example, we won't be able to accept a component that adds noise to the features. Think about unit tests. We prefer to standardise unit tests as much as possible but there may be specific things you'd like to check for.","title":"Adding a new Component"},{"location":"contributing/#testing","text":"To run the tests locally you'll need to run a script beforehand. python tests/prepare_everything.py This will prepare the filesystem for testing. We do this to prevent the need of downloading very large word embeddings locally and in CI. Fasttext can be 6-7 GB and we don't want to pull such a payload at every CI step. You can also prepare files locally by installing all dependencies via the Makefile . make install You can also run all style and type checking mechanisms locally via the Makefile . make check","title":"Testing"},{"location":"docs/featurizer/bytepair/","text":"This featurizer is a dense featurizer. If you're interested in learning how these work you might appreciate reading the original article . Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. A main feature of these types of embeddings is that they are relatively lightweight but also their availability in many languages. BytePair embeddings exist for 277 languages that are pretrained on wikipedia. There's also availability for a multi-language setting. More information can be found here . When you scroll down you will notice a list of languages that are available. Here's some links for a detailed view of available vectors: English Zulu Esperanto Afrikaans Hungarian Multi Language Configurable Variables \u00b6 lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25, vs : specifies the vocabulary size of the segmentation model, default = 1000, vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default=True, cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default=None, emb_file : specifies the path to a custom embedding file, default=None Base Usage \u00b6 The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy Cached Usage \u00b6 If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy Note that in this case we expect two files to be present in the tests/data directory; en.wiki.bpe.vs10000.d100.w2v.bin en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file and emb_file settings. But it is preferable to stick to the library naming convention.","title":"BytePairFeaturizer"},{"location":"docs/featurizer/bytepair/#configurable-variables","text":"lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25, vs : specifies the vocabulary size of the segmentation model, default = 1000, vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default=True, cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default=None, emb_file : specifies the path to a custom embedding file, default=None","title":"Configurable Variables"},{"location":"docs/featurizer/bytepair/#base-usage","text":"The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"Base Usage"},{"location":"docs/featurizer/bytepair/#cached-usage","text":"If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy Note that in this case we expect two files to be present in the tests/data directory; en.wiki.bpe.vs10000.d100.w2v.bin en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file and emb_file settings. But it is preferable to stick to the library naming convention.","title":"Cached Usage"},{"location":"docs/featurizer/fasttext/","text":"Here's an example configuration file that demonstrates how the fasttext featurizer works. Note that this featurizer is a dense featurizer. Fasttext supports 157 languages and you can download the embeddings here . Note that these files unzipped can be about 6-7Gb. Configurable Variables \u00b6 cache_dir : pass it the name of the directory where you've downloaded the embeddings file : pass it the name of the file that contains the word embeddings Base Usage \u00b6 The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_dir : downloaded/beforehand file : cc.nl.300.bin - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"FastTextFeaturizer"},{"location":"docs/featurizer/fasttext/#configurable-variables","text":"cache_dir : pass it the name of the directory where you've downloaded the embeddings file : pass it the name of the file that contains the word embeddings","title":"Configurable Variables"},{"location":"docs/featurizer/fasttext/#base-usage","text":"The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_dir : downloaded/beforehand file : cc.nl.300.bin - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"Base Usage"},{"location":"docs/meta/printer/","text":"Here's an example configuration file that demonstrates how the custom printer component works. You can find a tutorial on the component here . Configurable Variables \u00b6 alias : gives an extra name to the component and adds an extra message that is printed Base Usage \u00b6 When running this example you'll notice that what the effect is of the CountVectorsFeaturizer . You should see print statements appear when you talk to the assistant. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.meta.Printer alias : before count vectors - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.meta.Printer alias : after count vectors - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"Printer"},{"location":"docs/meta/printer/#configurable-variables","text":"alias : gives an extra name to the component and adds an extra message that is printed","title":"Configurable Variables"},{"location":"docs/meta/printer/#base-usage","text":"When running this example you'll notice that what the effect is of the CountVectorsFeaturizer . You should see print statements appear when you talk to the assistant. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.meta.Printer alias : before count vectors - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.meta.Printer alias : after count vectors - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"Base Usage"}]}