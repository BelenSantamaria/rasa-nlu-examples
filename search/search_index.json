{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rasa NLU Examples \u00b6 This repository contains some example components meant for educational/inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests as well as some documentation but this project should be considered a community project, not something that is part of core Rasa. Components \u00b6 The following components are implemented. Meta \u00b6 rasa_nlu_examples.meta.Printer docs \u00b6 This component will print what each featurizer adds to the NLU message. Very useful for debugging. Dense Featurizers \u00b6 rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs \u00b6 These are the pretrained embeddings from FastText, see for more info here . These are available in 157 languages, see here . rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs \u00b6 These BytePair embeddings are specialized subword embeddings that are built to be lightweight. See this link for more information. These are available in 227 languages and you can specify the subword vocabulary size as well as the dimensionality.","title":"Home"},{"location":"#rasa-nlu-examples","text":"This repository contains some example components meant for educational/inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests as well as some documentation but this project should be considered a community project, not something that is part of core Rasa.","title":"Rasa NLU Examples"},{"location":"#components","text":"The following components are implemented.","title":"Components"},{"location":"#meta","text":"","title":"Meta"},{"location":"#rasa_nlu_examplesmetaprinter-docs","text":"This component will print what each featurizer adds to the NLU message. Very useful for debugging.","title":"rasa_nlu_examples.meta.Printer docs"},{"location":"#dense-featurizers","text":"","title":"Dense Featurizers"},{"location":"#rasa_nlu_examplesfeaturizersdensefasttextfeaturizer-docs","text":"These are the pretrained embeddings from FastText, see for more info here . These are available in 157 languages, see here .","title":"rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs"},{"location":"#rasa_nlu_examplesfeaturizersdensebytepairfeaturizer-docs","text":"These BytePair embeddings are specialized subword embeddings that are built to be lightweight. See this link for more information. These are available in 227 languages and you can specify the subword vocabulary size as well as the dimensionality.","title":"rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs"},{"location":"benchmarking/","text":"Benchmarking Guide \u00b6 This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files like this one; language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer OOV_token : oov.txt token_pattern : (?u)\\b\\w+\\b - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 - name : DIETClassifier epochs : 200 Final Reminder \u00b6 There's one thing to remind ourselves of at this phase in time. We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline and the components that we host here may very well lag behind a version of Rasa.","title":"Benchmarking Guide"},{"location":"benchmarking/#benchmarking-guide","text":"This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files like this one; language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer OOV_token : oov.txt token_pattern : (?u)\\b\\w+\\b - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 - name : DIETClassifier epochs : 200","title":"Benchmarking Guide"},{"location":"benchmarking/#final-reminder","text":"There's one thing to remind ourselves of at this phase in time. We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline and the components that we host here may very well lag behind a version of Rasa.","title":"Final Reminder"},{"location":"docs/featurizer/bytepair/","text":"Here's an example configuration file that demonstrates how the byte-pair featurizer works. Note that this featurizer is a dense featurizer. If you're interested in learning how these works you might appreciate the article . BytePair embeddings exist for 277 languages and are also available for a multi-language setting. More information can be found here . For example, you can find an example of the detailed view for the Zulu language here . A main feature of these types of embeddings is that they are relatively lightweight. Configurable Variables \u00b6 lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25, vs : specifies the vocabulary size of the segmentation model, default = 1000, vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default=True, cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default=None, emb_file : specifies the path to a custom embedding file, default=None Base Usage \u00b6 The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy Cached Usage \u00b6 If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy Note that in this case we expect two files to be present in the tests/data directory; en.wiki.bpe.vs10000.d100.w2v.bin en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file and emb_file settings. But it is preferable to stick to the library naming convention.","title":"BytePairFeaturizer"},{"location":"docs/featurizer/bytepair/#configurable-variables","text":"lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25, vs : specifies the vocabulary size of the segmentation model, default = 1000, vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default=True, cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default=None, emb_file : specifies the path to a custom embedding file, default=None","title":"Configurable Variables"},{"location":"docs/featurizer/bytepair/#base-usage","text":"The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"Base Usage"},{"location":"docs/featurizer/bytepair/#cached-usage","text":"If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy Note that in this case we expect two files to be present in the tests/data directory; en.wiki.bpe.vs10000.d100.w2v.bin en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file and emb_file settings. But it is preferable to stick to the library naming convention.","title":"Cached Usage"},{"location":"docs/featurizer/fasttext/","text":"Here's an example configuration file that demonstrates how the fasttext featurizer works. Note that this featurizer is a dense featurizer. Fasttext supports 157 languages and you can download the embeddings here . Note that these files unzipped can be about 6-7Gb. Configurable Variables \u00b6 cache_dir : pass it the name of the directory where you've downloaded the embeddings file : pass it the name the file that contains the word embeddings Base Usage \u00b6 The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_dir : downloaded/beforehand file : cc.nl.300.bin - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"FastTextFeaturizer"},{"location":"docs/featurizer/fasttext/#configurable-variables","text":"cache_dir : pass it the name of the directory where you've downloaded the embeddings file : pass it the name the file that contains the word embeddings","title":"Configurable Variables"},{"location":"docs/featurizer/fasttext/#base-usage","text":"The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_dir : downloaded/beforehand file : cc.nl.300.bin - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"Base Usage"},{"location":"docs/meta/printer/","text":"Here's an example configuration file that demonstrates how the custom printer component works. Configurable Variables \u00b6 alias : gives an extra name to the component and adds an extra message that is printed Base Usage \u00b6 When running this example you'll notice that what the effect is of the CountVectorsFeaturizer . You should see print statements appear when you talk to the assistant. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.meta.Printer alias : before count vectors - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.meta.Printer alias : after count vectors - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"Printer"},{"location":"docs/meta/printer/#configurable-variables","text":"alias : gives an extra name to the component and adds an extra message that is printed","title":"Configurable Variables"},{"location":"docs/meta/printer/#base-usage","text":"When running this example you'll notice that what the effect is of the CountVectorsFeaturizer . You should see print statements appear when you talk to the assistant. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.meta.Printer alias : before count vectors - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.meta.Printer alias : after count vectors - name : DIETClassifier epochs : 100 policies : - name : MemoizationPolicy - name : KerasPolicy - name : MappingPolicy","title":"Base Usage"}]}