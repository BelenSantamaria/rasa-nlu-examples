{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Rasa NLU Examples \u00b6 This repository contains some example components meant for educational and inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests and some documentation but this is a community project, not something that is part of core Rasa. The goal of these tools will be to be compatible with the most recent version of rasa only. You may need to point to an older release of the project if you want it to be compatible with an older version of Rasa. The following components are implemented. Tokenizers \u00b6 Tokenizers can split up the input text into tokens. Depending on the Tokenizer that you pick you can also choose to apply lemmatization. For languages that have rich grammatical features this might help reduce the size of all the possible tokens. rasa_nlu_examples.tokenizers.BlankSpacyTokenizer docs rasa_nlu_examples.tokenizers.StanzaTokenizer docs rasa_nlu_examples.tokenizers.ThaiTokenizer docs Featurizers \u00b6 Dense featurizers attach dense numeric features per token as well as to the entire utterance. These features are picked up by intent classifiers and entity detectors later in the pipeline. rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs rasa_nlu_examples.featurizers.dense.GensimFeaturizer docs rasa_nlu_examples.featurizers.sparse.SparseBytePairFeaturizer docs rasa_nlu_examples.featurizers.sparse.SemanticMapFeaturizer docs Intent Classifiers \u00b6 Intent classifiers are models that predict an intent from a given user message text. The default intent classifier in Rasa NLU is the DIET model which can be fairly computationally expensive, especially if you do not need to detect entities. We provide some examples of alternative intent classifiers here. rasa_nlu_examples.classifiers.SparseNaiveBayesIntentClassifier docs Entity Extractors \u00b6 rasa_nlu_examples.extractor.FlashTextEntityExtractor docs rasa_nlu_examples.extractor.DateparserEntityExtractor docs Fallback Classifiers \u00b6 rasa_nlu_examples.fallback.FasttextLanguageFallbackClassifier docs Meta \u00b6 The components listed here won't effect the NLU pipeline but they might instead cause extra logs to appear to help with debugging. rasa_nlu_examples.meta.Printer docs rasa_nlu_examples.scikit.RasaClassifier docs rasa_nlu_examples.scikit.dataframe_to_nlu_file docs rasa_nlu_examples.scikit.nlu_path_to_dataframe docs Name Lists \u00b6 Language models in spaCy are typically trained on Western news datasets. That means that the reported benchmarks might not apply to your use-case. For example; detecting names in texts from France is not the same thing as detecting names in Madagascar. Even thought French is used actively in both countries, the names of it's citizens might be so different that you cannot assume that the benchmarks apply universally. To remedy this we've started collecting name lists. These can be used as a lookup table which can be picked up by Rasa's RegexEntityExtractor or our FlashTextEntityExtractor . It won't be 100% perfect but it should give a reasonable starting point. You can find the namelists here . We currently offer namelists for the United States, Germany as well as common Arabic names. Feel free to submit PRs for more languages. We're also eager to receive feedback. Contributing \u00b6 You can find the contribution guide here .","title":"Home"},{"location":"#rasa-nlu-examples","text":"This repository contains some example components meant for educational and inspirational purposes. These are components that we open source to encourage experimentation but these are components that are not officially supported . There will be some tests and some documentation but this is a community project, not something that is part of core Rasa. The goal of these tools will be to be compatible with the most recent version of rasa only. You may need to point to an older release of the project if you want it to be compatible with an older version of Rasa. The following components are implemented.","title":"Rasa NLU Examples"},{"location":"#tokenizers","text":"Tokenizers can split up the input text into tokens. Depending on the Tokenizer that you pick you can also choose to apply lemmatization. For languages that have rich grammatical features this might help reduce the size of all the possible tokens. rasa_nlu_examples.tokenizers.BlankSpacyTokenizer docs rasa_nlu_examples.tokenizers.StanzaTokenizer docs rasa_nlu_examples.tokenizers.ThaiTokenizer docs","title":"Tokenizers"},{"location":"#featurizers","text":"Dense featurizers attach dense numeric features per token as well as to the entire utterance. These features are picked up by intent classifiers and entity detectors later in the pipeline. rasa_nlu_examples.featurizers.dense.FastTextFeaturizer docs rasa_nlu_examples.featurizers.dense.BytePairFeaturizer docs rasa_nlu_examples.featurizers.dense.GensimFeaturizer docs rasa_nlu_examples.featurizers.sparse.SparseBytePairFeaturizer docs rasa_nlu_examples.featurizers.sparse.SemanticMapFeaturizer docs","title":"Featurizers"},{"location":"#intent-classifiers","text":"Intent classifiers are models that predict an intent from a given user message text. The default intent classifier in Rasa NLU is the DIET model which can be fairly computationally expensive, especially if you do not need to detect entities. We provide some examples of alternative intent classifiers here. rasa_nlu_examples.classifiers.SparseNaiveBayesIntentClassifier docs","title":"Intent Classifiers"},{"location":"#entity-extractors","text":"rasa_nlu_examples.extractor.FlashTextEntityExtractor docs rasa_nlu_examples.extractor.DateparserEntityExtractor docs","title":"Entity Extractors"},{"location":"#fallback-classifiers","text":"rasa_nlu_examples.fallback.FasttextLanguageFallbackClassifier docs","title":"Fallback Classifiers"},{"location":"#meta","text":"The components listed here won't effect the NLU pipeline but they might instead cause extra logs to appear to help with debugging. rasa_nlu_examples.meta.Printer docs rasa_nlu_examples.scikit.RasaClassifier docs rasa_nlu_examples.scikit.dataframe_to_nlu_file docs rasa_nlu_examples.scikit.nlu_path_to_dataframe docs","title":"Meta"},{"location":"#name-lists","text":"Language models in spaCy are typically trained on Western news datasets. That means that the reported benchmarks might not apply to your use-case. For example; detecting names in texts from France is not the same thing as detecting names in Madagascar. Even thought French is used actively in both countries, the names of it's citizens might be so different that you cannot assume that the benchmarks apply universally. To remedy this we've started collecting name lists. These can be used as a lookup table which can be picked up by Rasa's RegexEntityExtractor or our FlashTextEntityExtractor . It won't be 100% perfect but it should give a reasonable starting point. You can find the namelists here . We currently offer namelists for the United States, Germany as well as common Arabic names. Feel free to submit PRs for more languages. We're also eager to receive feedback.","title":"Name Lists"},{"location":"#contributing","text":"You can find the contribution guide here .","title":"Contributing"},{"location":"benchmarking/","text":"Benchmarking Guide \u00b6 This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files with NLU components from this library. You can glance over some examples below. Basic Config Here's a very basic configuartion file. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: DIETClassifier epochs: 200 Assuming this file is named basic-config.yml you can run this pipeline as a benchmark by running this command from the project directory; rasa test nlu --config basic-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-config This will generate output in the gridresults/basic-config folder. Basic Byte-Pair Here's the same basic configuration but now with dense features added. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 1000 dim: 25 - name: DIETClassifier epochs: 200 Assuming this file is named basic-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config basic-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-bytepair-config This will generate output in the gridresults/basic-bytepair-config folder. Medium Byte-Pair We've now increased the vocabulary size and dimensionality. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 10000 dim: 100 - name: DIETClassifier epochs: 200 Assuming this file is named medium-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config medium-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/medium-bytepair-config This will generate output in the gridresults/medium-bytepair-config folder. Large Byte-Pair We've now grabbed the largest English Byte-Pair embeddings available. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 200000 dim: 300 - name: DIETClassifier epochs: 200 Assuming this file is named large-bytepair-config.yml you can run this benchmark by running this command from the project directory; rasa test nlu --config large-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/large-bytepair-config This will generate output in the gridresults/large-bytepair-config folder. Final Reminder \u00b6 We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline. The components that we host here may very well lag behind Rasa Open Source too.","title":"Benchmarking Guide"},{"location":"benchmarking/#benchmarking-guide","text":"This is a small guide that will explain how you can use the tools in this library to run benchmarks. As an example project we'll use the Sara demo . First you'll need to install the project. An easy way to do this is via pip; pip install git+https://github.com/RasaHQ/rasa-nlu-examples You should now be able to run configuration files with NLU components from this library. You can glance over some examples below. Basic Config Here's a very basic configuartion file. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: DIETClassifier epochs: 200 Assuming this file is named basic-config.yml you can run this pipeline as a benchmark by running this command from the project directory; rasa test nlu --config basic-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-config This will generate output in the gridresults/basic-config folder. Basic Byte-Pair Here's the same basic configuration but now with dense features added. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 1000 dim: 25 - name: DIETClassifier epochs: 200 Assuming this file is named basic-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config basic-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/basic-bytepair-config This will generate output in the gridresults/basic-bytepair-config folder. Medium Byte-Pair We've now increased the vocabulary size and dimensionality. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 10000 dim: 100 - name: DIETClassifier epochs: 200 Assuming this file is named medium-bytepair-config.yml you can run it as a benchmark by running this command from the project directory; rasa test nlu --config medium-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/medium-bytepair-config This will generate output in the gridresults/medium-bytepair-config folder. Large Byte-Pair We've now grabbed the largest English Byte-Pair embeddings available. language: en pipeline: - name: WhitespaceTokenizer - name: CountVectorsFeaturizer OOV_token: oov.txt analyzer: word - name: CountVectorsFeaturizer analyzer: char_wb min_ngram: 1 max_ngram: 4 - name: rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang: en vs: 200000 dim: 300 - name: DIETClassifier epochs: 200 Assuming this file is named large-bytepair-config.yml you can run this benchmark by running this command from the project directory; rasa test nlu --config large-bytepair-config.yml \\ --cross-validation --runs 1 --folds 2 \\ --out gridresults/large-bytepair-config This will generate output in the gridresults/large-bytepair-config folder.","title":"Benchmarking Guide"},{"location":"benchmarking/#final-reminder","text":"We should remember that these tools are experimental in nature. We want this repository to be a place where folks can share their nlu components and experiment, but this also means that we don't want to suggest that these tools are state of the art. You always need to check if these tools work for your pipeline. The components that we host here may very well lag behind Rasa Open Source too.","title":"Final Reminder"},{"location":"contributing/","text":"Contributing Guide \u00b6 Ways you can Contribute \u00b6 We're open to contributions and there are many ways that you can make one. You can suggest new features. You can help review new features. You can submit new components. You can let us know if there are bugs. You can let us know if the components in this library help you. Open an Issue \u00b6 You can submit an issue here . Issues allow us to keep track of a conversation about this repository and it is the preferred communication channel for bugs related to this project. Suggest a New Feature \u00b6 This project started because we wanted to offer support for word embeddings for more languages. The first feature we added was support for FastText, which offers embeddings for 157 languages. We later received a contribution from a community member for BytePair embeddings, which offers support for 275 languages. We weren't aware of these embeddings but we were exited to support more languages. Odds are that there are many more tools out there that the maintainers of this project aren't aware of yet. There may very well be more embeddings, tokenziers, lemmatizers and models that we're not aware of but can help Rasa developers make better assistants. The goal of this project is to support more of these sorts of tools for Rasa users. You can help out the project just by letting us know if there's an integration missing. If you do not have the time to contribute a component yourself then you can still contribute to the effort by letting us know what components might help you make a better assistant. Share an Observation \u00b6 If the tools that we offer here turn out to be useful then we'd love to hear about it. We're also interested in hearing if these tools don't work for your usecase. Any feedback will be shared with the research team at Rasa. We're especially keen to hear feedback on the performance of the word embeddings that we host here. You can leave a message either on the github issue list or on the Rasa forum . Be sure to ping koaning on the forum if you mention this project, he's the main maintainer. Adding a new Component \u00b6 There's a balance between allowing experimentation and maintaining all the code. This is why we've come up with a checklist that you should keep in mind before you're submitting code. If you want to contribute a new component please make an issue first so we can discuss it. We want to prevent double work where possible and make sure the proposed component is appropriate for this repository. New tools that are added here need to be plausibly useful in a real life scenario. For example, we won't be able to accept a component that adds noise to the features. Think about unit tests. We prefer to standardise unit tests as much as possible but there may be specific things you'd like to check for. Ask for help! Feel free to ping the koaning on this repository if you're having trouble with an implementation or appreciate guidance on a topic. He'll gladly help you. Testing \u00b6 We run automated tests via GitHub actions but you can also run all the checking mechanisms locally. To run the tests locally you'll need to run a few scripts beforehand. python tests/scripts/prepare_fasttext.py python tests/scripts/prepare_stanza.py This will prepare the filesystem for testing. We do this to prevent the need of downloading very large word embeddings locally and in CI. Fasttext can be 6-7 GB and we don't want to pull such a payload at every CI step. You can also prepare files locally by installing all dependencies via the Makefile . make install You can also run all style and type checking mechanisms locally via the Makefile . make check","title":"Contributing"},{"location":"contributing/#contributing-guide","text":"","title":"Contributing Guide"},{"location":"contributing/#ways-you-can-contribute","text":"We're open to contributions and there are many ways that you can make one. You can suggest new features. You can help review new features. You can submit new components. You can let us know if there are bugs. You can let us know if the components in this library help you.","title":"Ways you can Contribute"},{"location":"contributing/#open-an-issue","text":"You can submit an issue here . Issues allow us to keep track of a conversation about this repository and it is the preferred communication channel for bugs related to this project.","title":"Open an Issue"},{"location":"contributing/#suggest-a-new-feature","text":"This project started because we wanted to offer support for word embeddings for more languages. The first feature we added was support for FastText, which offers embeddings for 157 languages. We later received a contribution from a community member for BytePair embeddings, which offers support for 275 languages. We weren't aware of these embeddings but we were exited to support more languages. Odds are that there are many more tools out there that the maintainers of this project aren't aware of yet. There may very well be more embeddings, tokenziers, lemmatizers and models that we're not aware of but can help Rasa developers make better assistants. The goal of this project is to support more of these sorts of tools for Rasa users. You can help out the project just by letting us know if there's an integration missing. If you do not have the time to contribute a component yourself then you can still contribute to the effort by letting us know what components might help you make a better assistant.","title":"Suggest a New Feature"},{"location":"contributing/#share-an-observation","text":"If the tools that we offer here turn out to be useful then we'd love to hear about it. We're also interested in hearing if these tools don't work for your usecase. Any feedback will be shared with the research team at Rasa. We're especially keen to hear feedback on the performance of the word embeddings that we host here. You can leave a message either on the github issue list or on the Rasa forum . Be sure to ping koaning on the forum if you mention this project, he's the main maintainer.","title":"Share an Observation"},{"location":"contributing/#adding-a-new-component","text":"There's a balance between allowing experimentation and maintaining all the code. This is why we've come up with a checklist that you should keep in mind before you're submitting code. If you want to contribute a new component please make an issue first so we can discuss it. We want to prevent double work where possible and make sure the proposed component is appropriate for this repository. New tools that are added here need to be plausibly useful in a real life scenario. For example, we won't be able to accept a component that adds noise to the features. Think about unit tests. We prefer to standardise unit tests as much as possible but there may be specific things you'd like to check for. Ask for help! Feel free to ping the koaning on this repository if you're having trouble with an implementation or appreciate guidance on a topic. He'll gladly help you.","title":"Adding a new Component"},{"location":"contributing/#testing","text":"We run automated tests via GitHub actions but you can also run all the checking mechanisms locally. To run the tests locally you'll need to run a few scripts beforehand. python tests/scripts/prepare_fasttext.py python tests/scripts/prepare_stanza.py This will prepare the filesystem for testing. We do this to prevent the need of downloading very large word embeddings locally and in CI. Fasttext can be 6-7 GB and we don't want to pull such a payload at every CI step. You can also prepare files locally by installing all dependencies via the Makefile . make install You can also run all style and type checking mechanisms locally via the Makefile . make check","title":"Testing"},{"location":"docs/classifier/sparsenb/","text":"SparseNaiveBayesIntentClassifier \u00b6 This intent classifier is based on the Bernoulli-variant of the Na\u00efve Bayes classifier in sklearn . This classifier only looks at sparse features extracted from the Rasa NLU feature pipeline and is a faster alternative to neural models like DIET . This model requires that there be some sparse featurizers in your pipeleine. If you config only has dense features it will throw an exception. Configurable Variables \u00b6 alpha (default: 1.0): Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). binarize (default: 0.0): Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors. fit_prior (default: True): Whether to learn class prior probabilities or not. If false, a uniform prior will be used. class_prior (default: None): Prior probabilities (as a list) of the classes. If specified the priors are not adjusted according to the data. Base Usage \u00b6 The configuration file below demonstrates how you might use the this component. In this example we are extracting sparse features with two CountVectorsFeaturizer instances, the first of which produces sparse bag-of-words features, and the second which produces sparse bags-of-character-ngram features. We've also set the alpha smoothing parameter to 0.1. language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifiers.SparseNaiveBayesIntentClassifier alpha : 0.1 Unlike DIET , this classifier only predicts intents. If you also need entity extraction, you will have to add a separate entity extractor to your config. Below is an example where we have included the CRFEntityExtractor to extract entities. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifiers.SparseNaiveBayesIntentClassifier alpha : 0.1 - name : CRFEntityExtractor","title":"SparseNaiveBayes"},{"location":"docs/classifier/sparsenb/#sparsenaivebayesintentclassifier","text":"This intent classifier is based on the Bernoulli-variant of the Na\u00efve Bayes classifier in sklearn . This classifier only looks at sparse features extracted from the Rasa NLU feature pipeline and is a faster alternative to neural models like DIET . This model requires that there be some sparse featurizers in your pipeleine. If you config only has dense features it will throw an exception.","title":"SparseNaiveBayesIntentClassifier"},{"location":"docs/classifier/sparsenb/#configurable-variables","text":"alpha (default: 1.0): Additive (Laplace/Lidstone) smoothing parameter (0 for no smoothing). binarize (default: 0.0): Threshold for binarizing (mapping to booleans) of sample features. If None, input is presumed to already consist of binary vectors. fit_prior (default: True): Whether to learn class prior probabilities or not. If false, a uniform prior will be used. class_prior (default: None): Prior probabilities (as a list) of the classes. If specified the priors are not adjusted according to the data.","title":"Configurable Variables"},{"location":"docs/classifier/sparsenb/#base-usage","text":"The configuration file below demonstrates how you might use the this component. In this example we are extracting sparse features with two CountVectorsFeaturizer instances, the first of which produces sparse bag-of-words features, and the second which produces sparse bags-of-character-ngram features. We've also set the alpha smoothing parameter to 0.1. language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifiers.SparseNaiveBayesIntentClassifier alpha : 0.1 Unlike DIET , this classifier only predicts intents. If you also need entity extraction, you will have to add a separate entity extractor to your config. Below is an example where we have included the CRFEntityExtractor to extract entities. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.classifiers.SparseNaiveBayesIntentClassifier alpha : 0.1 - name : CRFEntityExtractor","title":"Base Usage"},{"location":"docs/extractors/dateparser/","text":"DateparserEntityExtractor \u00b6 Note If you want to use this component, be sure to either install flashtext manually or use our convenience installer. python -m pip install \"rasa_nlu_examples[dateparser] @ git+https://github.com/RasaHQ/rasa-nlu-examples.git\" What does it do? \u00b6 This entity extractor uses the dateparser to extract entities that resemble dates. You can get a demo by running the code below. from rasa.shared.nlu.training_data.message import Message from rasa_nlu_examples.extractors.dateparser_extractor import DateparserEntityExtractor from rich import print msg = Message . build ( \"hello tomorrow, goodbye yesterday\" ,) extractor = DateparserEntityExtractor ({}) extractor . process ( msg ) print ( msg . as_dict_nlu ()) This will parse the following information. { 'text' : 'hello tomorrow, goodbye yesterday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 6 , 'end' : 14 , 'value' : 'tomorrow' , 'parsed_date' : '2021-06-05 11:50:10.502082' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' }, { 'entity' : 'DATETIME_REFERENCE' , 'start' : 24 , 'end' : 33 , 'value' : 'yesterday' , 'parsed_date' : '2021-06-03 11:50:10.503160' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } Note that we add an extra parsed_date key to the entity dictionary here. Another benefit of dateparser is that it also contains rules for Non-English languages. Here is a Dutch example. { 'text' : 'ik wil een pizza bestellen voor morgen' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 32 , 'end' : 38 , 'value' : 'morgen' , 'parsed_date' : '2021-06-05 11:50:10.708588' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } It's also possible to configure the DateparserEntityExtractor to prefer dates in the future or in the past. That way, if somebody talks about Thursday can be picked up as next Thursday, allowing us to still parse out a date. \"Future\" Results \u00b6 This ran on Friday the 4th of June, 2021. { 'text' : 'i want a pizza thursday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 15 , 'end' : 23 , 'value' : 'thursday' , 'parsed_date' : '2021-06-10 00:00:00' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } \"Past\" Results \u00b6 This ran on Friday the 4th of June, 2021. { 'text' : 'i want to buy a pizza thursday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 22 , 'end' : 30 , 'value' : 'thursday' , 'parsed_date' : '2021-06-03 00:00:00' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } Configurable Variables \u00b6 languages : pass a list of languages that you want the parser to focus on, can be None but this setting is likely to overfit on English assumptions prefer_dates_from : can be either \"future\", \"past or None Base Usage \u00b6 The configuration below is an example of how you might use FlashTextEntityExtractor . language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 - name : rasa_nlu_examples.extractors.DateparserEntityExtractor languages : [ \"en\" , \"nl\" , \"es\" ] prefer_dates_from : \"future\" Note that this entity extractor completely ignores the tokeniser. There might also be overlap with enities from other engines, like DIET and spaCy.","title":"DateParser"},{"location":"docs/extractors/dateparser/#dateparserentityextractor","text":"Note If you want to use this component, be sure to either install flashtext manually or use our convenience installer. python -m pip install \"rasa_nlu_examples[dateparser] @ git+https://github.com/RasaHQ/rasa-nlu-examples.git\"","title":"DateparserEntityExtractor"},{"location":"docs/extractors/dateparser/#what-does-it-do","text":"This entity extractor uses the dateparser to extract entities that resemble dates. You can get a demo by running the code below. from rasa.shared.nlu.training_data.message import Message from rasa_nlu_examples.extractors.dateparser_extractor import DateparserEntityExtractor from rich import print msg = Message . build ( \"hello tomorrow, goodbye yesterday\" ,) extractor = DateparserEntityExtractor ({}) extractor . process ( msg ) print ( msg . as_dict_nlu ()) This will parse the following information. { 'text' : 'hello tomorrow, goodbye yesterday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 6 , 'end' : 14 , 'value' : 'tomorrow' , 'parsed_date' : '2021-06-05 11:50:10.502082' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' }, { 'entity' : 'DATETIME_REFERENCE' , 'start' : 24 , 'end' : 33 , 'value' : 'yesterday' , 'parsed_date' : '2021-06-03 11:50:10.503160' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } Note that we add an extra parsed_date key to the entity dictionary here. Another benefit of dateparser is that it also contains rules for Non-English languages. Here is a Dutch example. { 'text' : 'ik wil een pizza bestellen voor morgen' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 32 , 'end' : 38 , 'value' : 'morgen' , 'parsed_date' : '2021-06-05 11:50:10.708588' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] } It's also possible to configure the DateparserEntityExtractor to prefer dates in the future or in the past. That way, if somebody talks about Thursday can be picked up as next Thursday, allowing us to still parse out a date.","title":"What does it do?"},{"location":"docs/extractors/dateparser/#future-results","text":"This ran on Friday the 4th of June, 2021. { 'text' : 'i want a pizza thursday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 15 , 'end' : 23 , 'value' : 'thursday' , 'parsed_date' : '2021-06-10 00:00:00' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] }","title":"\"Future\" Results"},{"location":"docs/extractors/dateparser/#past-results","text":"This ran on Friday the 4th of June, 2021. { 'text' : 'i want to buy a pizza thursday' , 'entities' : [ { 'entity' : 'DATETIME_REFERENCE' , 'start' : 22 , 'end' : 30 , 'value' : 'thursday' , 'parsed_date' : '2021-06-03 00:00:00' , 'confidence' : 1.0 , 'extractor' : 'DateparserEntityExtractor' } ] }","title":"\"Past\" Results"},{"location":"docs/extractors/dateparser/#configurable-variables","text":"languages : pass a list of languages that you want the parser to focus on, can be None but this setting is likely to overfit on English assumptions prefer_dates_from : can be either \"future\", \"past or None","title":"Configurable Variables"},{"location":"docs/extractors/dateparser/#base-usage","text":"The configuration below is an example of how you might use FlashTextEntityExtractor . language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 - name : rasa_nlu_examples.extractors.DateparserEntityExtractor languages : [ \"en\" , \"nl\" , \"es\" ] prefer_dates_from : \"future\" Note that this entity extractor completely ignores the tokeniser. There might also be overlap with enities from other engines, like DIET and spaCy.","title":"Base Usage"},{"location":"docs/extractors/flashtext/","text":"FlashTextEntityExtractor \u00b6 Note If you want to use this component, be sure to either install flashtext manually or use our convenience installer. python -m pip install \"rasa_nlu_examples[flashtext] @ git+https://github.com/RasaHQ/rasa-nlu-examples.git\" This entity extractor uses the flashtext library to extract entities using lookup tables . This is similar to RegexEntityExtractor , but different in a few ways: FlashTextEntityExtractor takes only lookups , not regex patterns FlashTextEntityExtractor matches using whitespace word boundaries. You cannot set it to match words regardless of boundaries. FlashTextEntityExtractor is much faster than RegexEntityExtractor . This is especially true for large lookup tables. Also note that anything other than [A-Za-z0-9_] is considered a word boundary. To add more non-word boundaries use the parameter non_word_boundaries Configurable Variables \u00b6 case_sensitive : whether to consider case when matching entities. False by default. non_word_boundaries : characters which shouldn't be considered word boundaries. Base Usage \u00b6 The configuration below is an example of how you might use FlashTextEntityExtractor . language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.extractors.FlashTextEntityExtractor case_sensitive : True non_word_boundary : - \"_\" - \",\" - name : DIETClassifier epochs : 100 You must include lookup tables in your NLU data. This might look like: nlu : - lookup : country examples : | - Afghanistan - Albania - ... - Zambia - Zimbabwe In this example, anytime a user's utterance contains an exact match for a country from the lookup table above, FlashTextEntityExtractor will extract this as an entity with type country . You should include a few examples with this entity in your intent data, like so: - intent : inform_home_country examples : | - I am from [Afghanistan](country) - My family is from [Albania](country","title":"FlashText"},{"location":"docs/extractors/flashtext/#flashtextentityextractor","text":"Note If you want to use this component, be sure to either install flashtext manually or use our convenience installer. python -m pip install \"rasa_nlu_examples[flashtext] @ git+https://github.com/RasaHQ/rasa-nlu-examples.git\" This entity extractor uses the flashtext library to extract entities using lookup tables . This is similar to RegexEntityExtractor , but different in a few ways: FlashTextEntityExtractor takes only lookups , not regex patterns FlashTextEntityExtractor matches using whitespace word boundaries. You cannot set it to match words regardless of boundaries. FlashTextEntityExtractor is much faster than RegexEntityExtractor . This is especially true for large lookup tables. Also note that anything other than [A-Za-z0-9_] is considered a word boundary. To add more non-word boundaries use the parameter non_word_boundaries","title":"FlashTextEntityExtractor"},{"location":"docs/extractors/flashtext/#configurable-variables","text":"case_sensitive : whether to consider case when matching entities. False by default. non_word_boundaries : characters which shouldn't be considered word boundaries.","title":"Configurable Variables"},{"location":"docs/extractors/flashtext/#base-usage","text":"The configuration below is an example of how you might use FlashTextEntityExtractor . language : en pipeline : - name : WhitespaceTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.extractors.FlashTextEntityExtractor case_sensitive : True non_word_boundary : - \"_\" - \",\" - name : DIETClassifier epochs : 100 You must include lookup tables in your NLU data. This might look like: nlu : - lookup : country examples : | - Afghanistan - Albania - ... - Zambia - Zimbabwe In this example, anytime a user's utterance contains an exact match for a country from the lookup table above, FlashTextEntityExtractor will extract this as an entity with type country . You should include a few examples with this entity in your intent data, like so: - intent : inform_home_country examples : | - I am from [Afghanistan](country) - My family is from [Albania](country","title":"Base Usage"},{"location":"docs/fallback/fasttextlanguagefallback/","text":"FasttextLanguageFallbackClassifier \u00b6 This classifier uses fasttext to detect if an unintended language is used. You can combine this tool together with RulePolicy rules to handle out of scope responses more elegantly. Assuming that you're making an assistant to handle English then you can send the user an appropriate response if this model predicts another language. The tool should be able to detect 176 languages but the predictions won't be perfect. Especially when the user sends short utterances we need to be careful. That is why this tool allows you to specify a minimum number of characters and tokens before this model triggers an intent. Note In order to use this tool you'll need to ensure the correct dependencies are installed. pip install \"rasa_nlu_examples[fasttext] @ https://github.com/RasaHQ/rasa-nlu-examples.git\" Understanding the Tool \u00b6 You're encouraged to play with the tool from a jupyter notebook so you can understand what kinds of mistakes the model might make. To do that you'll first want to download the fasttext library as well as the pretrained model. python -m pip install fasttext wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz Then from a notebook you should be able to interact with the model by running: import fasttext model = fasttext . load_model ( \"lid.176.ftz\" ) txt = \"i am speaking english\" proba_dict = { k : v for k , v in zip ( * model . predict ( txt , k = 10 ))} proba_dict Configurable Variables \u00b6 expected_language (required): the language that you expect to be predicted. If this language is predicted we won't trigger an intent. intent_triggered (required): the name of the intent to trigger when the model does not detect the expected language. cache_dir (required): specifies the folder where the pretrained model can be found. model_file (required): specifies the path to a pretrained model file, typically you'll want lid.176.ftz . See the fasttext docs for more info. threshold (default: 0.7): if the probability for your language is smaller than this threshold then we trigger the intent. min_tokens (default: 3): the minimum number of tokens that need to be in the utterance. If there's less tokens the language model is ignored because it is likely to be in-accurate. min_chars (default: 10): the minimum number of characters of text that need to be in the utterance. If there's less tokens the language model is ignored because it is likely to be in-accurate. protected_intents (default: [] ): specifies a list of intent names that won't be overwritten Base Usage \u00b6 The configuration file below demonstrates how you might use the this component. In this example we've assumed that you've downloaded the lightweight \"lid.176.ftz\" model beforehand and that it exists in the downloaded folder that is on the root path of your project. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 1 - name : rasa_nlu_examples.fallback.FasttextLanguageFallbackClassifier expected_language : en intent_triggered : non_english cache_dir : downloaded file : 'lid.176.ftz' min_chars : 5 min_tokens : 2 threshold : 0.3 protected_intents : [ \"greet\" ] To get the most out of this tool you also need to add a rule to a rules.yml file. That way you can configure an appropriate action whenever a user is speaking in the wrong language. That might look something like this: rules : - rule : Ask the user to switch to speaking in English. steps : - intent : non_english - action : utter_non_english For more information on rules, see the docs .","title":"FasttextLanguage"},{"location":"docs/fallback/fasttextlanguagefallback/#fasttextlanguagefallbackclassifier","text":"This classifier uses fasttext to detect if an unintended language is used. You can combine this tool together with RulePolicy rules to handle out of scope responses more elegantly. Assuming that you're making an assistant to handle English then you can send the user an appropriate response if this model predicts another language. The tool should be able to detect 176 languages but the predictions won't be perfect. Especially when the user sends short utterances we need to be careful. That is why this tool allows you to specify a minimum number of characters and tokens before this model triggers an intent. Note In order to use this tool you'll need to ensure the correct dependencies are installed. pip install \"rasa_nlu_examples[fasttext] @ https://github.com/RasaHQ/rasa-nlu-examples.git\"","title":"FasttextLanguageFallbackClassifier"},{"location":"docs/fallback/fasttextlanguagefallback/#understanding-the-tool","text":"You're encouraged to play with the tool from a jupyter notebook so you can understand what kinds of mistakes the model might make. To do that you'll first want to download the fasttext library as well as the pretrained model. python -m pip install fasttext wget https://dl.fbaipublicfiles.com/fasttext/supervised-models/lid.176.ftz Then from a notebook you should be able to interact with the model by running: import fasttext model = fasttext . load_model ( \"lid.176.ftz\" ) txt = \"i am speaking english\" proba_dict = { k : v for k , v in zip ( * model . predict ( txt , k = 10 ))} proba_dict","title":"Understanding the Tool"},{"location":"docs/fallback/fasttextlanguagefallback/#configurable-variables","text":"expected_language (required): the language that you expect to be predicted. If this language is predicted we won't trigger an intent. intent_triggered (required): the name of the intent to trigger when the model does not detect the expected language. cache_dir (required): specifies the folder where the pretrained model can be found. model_file (required): specifies the path to a pretrained model file, typically you'll want lid.176.ftz . See the fasttext docs for more info. threshold (default: 0.7): if the probability for your language is smaller than this threshold then we trigger the intent. min_tokens (default: 3): the minimum number of tokens that need to be in the utterance. If there's less tokens the language model is ignored because it is likely to be in-accurate. min_chars (default: 10): the minimum number of characters of text that need to be in the utterance. If there's less tokens the language model is ignored because it is likely to be in-accurate. protected_intents (default: [] ): specifies a list of intent names that won't be overwritten","title":"Configurable Variables"},{"location":"docs/fallback/fasttextlanguagefallback/#base-usage","text":"The configuration file below demonstrates how you might use the this component. In this example we've assumed that you've downloaded the lightweight \"lid.176.ftz\" model beforehand and that it exists in the downloaded folder that is on the root path of your project. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 1 - name : rasa_nlu_examples.fallback.FasttextLanguageFallbackClassifier expected_language : en intent_triggered : non_english cache_dir : downloaded file : 'lid.176.ftz' min_chars : 5 min_tokens : 2 threshold : 0.3 protected_intents : [ \"greet\" ] To get the most out of this tool you also need to add a rule to a rules.yml file. That way you can configure an appropriate action whenever a user is speaking in the wrong language. That might look something like this: rules : - rule : Ask the user to switch to speaking in English. steps : - intent : non_english - action : utter_non_english For more information on rules, see the docs .","title":"Base Usage"},{"location":"docs/featurizer/bytepair/","text":"This featurizer is a dense featurizer. If you're interested in learning how these work you might appreciate reading the original article . Recognition should be given to Benjamin Heinzerling and Michael Strube for making these available. A main feature of these types of embeddings is that they are relatively lightweight but also their availability in many languages. BytePair embeddings exist for 277 languages that are pretrained on wikipedia. There's also availability for a multi-language setting. More information on these embeddings can be found here . When you scroll down you will notice a large of languages that are available. Here's some examples from that list that give a detailed view of available vectors: Abkhazian Zulu English Hindi Chinese Esperanto Multi Language Configurable Variables \u00b6 lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25 , vs : specifies the vocabulary size of the segmentation model, default = 1000 , vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default= True cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default= None , emb_file : specifies the path to a custom embedding file, default= None Base Usage \u00b6 The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100 Cached Usage \u00b6 If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 Note that in this case we expect two files to be present in the tests/data directory; en.wiki.bpe.vs10000.d100.w2v.bin en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file and emb_file settings. But it is preferable to stick to the library naming convention. Also note that if you use the model_file and emb_file settings that you must provide full filepaths and that the cache_dir will be ignored. It is still considered good practice to manually specify the lang , dim and vs parameter in this situation.","title":"BytePairFeaturizer"},{"location":"docs/featurizer/bytepair/#configurable-variables","text":"lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25 , vs : specifies the vocabulary size of the segmentation model, default = 1000 , vs_fallback : if set to True and the given vocabulary size can't be loaded for the given model, the closest size is chosen, default= True cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default= None , emb_file : specifies the path to a custom embedding file, default= None","title":"Configurable Variables"},{"location":"docs/featurizer/bytepair/#base-usage","text":"The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 dim : 25 - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/bytepair/#cached-usage","text":"If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 10000 dim : 100 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 Note that in this case we expect two files to be present in the tests/data directory; en.wiki.bpe.vs10000.d100.w2v.bin en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file and emb_file settings. But it is preferable to stick to the library naming convention. Also note that if you use the model_file and emb_file settings that you must provide full filepaths and that the cache_dir will be ignored. It is still considered good practice to manually specify the lang , dim and vs parameter in this situation.","title":"Cached Usage"},{"location":"docs/featurizer/fasttext/","text":"Fasttext supports word embeddings for 157 languages and is trained on both Common Crawl and Wikipedia. You can download the embeddings here . Note that this featurizer is a dense featurizer. Beware that these embedding files tend to be big: about 6-7Gb. Note In order to use this tool you'll need to ensure the correct dependencies are installed. pip install \"rasa_nlu_examples[fasttext] @ https://github.com/RasaHQ/rasa-nlu-examples.git\" Configurable Variables \u00b6 cache_dir : pass it the name of the directory where you've downloaded the embeddings file : pass it the name of the file that contains the word embeddings Base Usage \u00b6 The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_dir : downloaded/beforehand file : cc.nl.300.bin - name : DIETClassifier epochs : 100","title":"FastTextFeaturizer"},{"location":"docs/featurizer/fasttext/#configurable-variables","text":"cache_dir : pass it the name of the directory where you've downloaded the embeddings file : pass it the name of the file that contains the word embeddings","title":"Configurable Variables"},{"location":"docs/featurizer/fasttext/#base-usage","text":"The configuration file below demonstrates how you might use the fasttext embeddings. In this example we're building a pipeline for the Dutch language and we're assuming that the embeddings have been downloaded beforehand and save over at downloaded/beforehand/cc.nl.300.bin . language : nl pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.FastTextFeaturizer cache_dir : downloaded/beforehand file : cc.nl.300.bin - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/gensim/","text":"This page discusses some properties of the GensimFeaturizer . Note that this featurizer is a dense featurizer. Gensim is a popular python library that makes it relatively easy to train your own word vectors. This can be useful if your corpus is very different than what most popular embeddings are trained on. We'll give a small guide on how to train your own embeddings here but you can also read the guide on the gensim docs . Training Your Own \u00b6 Training your own gensim model can be done in a few lines of code. A demonstration is shown below. from gensim.models import Word2Vec # Gensim needs a list of lists to represent tokens in a document. # In real life you\u2019d read a text file and turn it into lists here. text = [ \"this is a sentence\" , \"so is this\" , \"and we're all talking\" ] tokens = [ t . split ( \" \" ) for t in text ] # This is where we train new word embeddings. model = Word2Vec ( sentences = tokens , size = 10 , window = 3 , min_count = 1 , iter = 5 , workers = 2 ) # This is where they are saved to disk. model . wv . save ( \"wordvectors.kv\" ) This wordvectors.kv file should contain all the vectors that you've trained. It's this file that you can pass on to this component. Configurable Variables \u00b6 cache_dir : pass it the name of the directory where you've downloaded/saved the embeddings file : pass it the name of the .kv file that contains the word embeddings Base Usage \u00b6 The configuration file below demonstrates how you might use the gensim embeddings. In this example we're building a pipeline for the English language and we're assuming that you've trained your own embeddings which have been saved upfront as saved/beforehand/filename.kv . language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.GensimFeaturizer cache_dir : saved/beforehand file : filename.kv - name : DIETClassifier epochs : 100","title":"GensimFeaturizer"},{"location":"docs/featurizer/gensim/#training-your-own","text":"Training your own gensim model can be done in a few lines of code. A demonstration is shown below. from gensim.models import Word2Vec # Gensim needs a list of lists to represent tokens in a document. # In real life you\u2019d read a text file and turn it into lists here. text = [ \"this is a sentence\" , \"so is this\" , \"and we're all talking\" ] tokens = [ t . split ( \" \" ) for t in text ] # This is where we train new word embeddings. model = Word2Vec ( sentences = tokens , size = 10 , window = 3 , min_count = 1 , iter = 5 , workers = 2 ) # This is where they are saved to disk. model . wv . save ( \"wordvectors.kv\" ) This wordvectors.kv file should contain all the vectors that you've trained. It's this file that you can pass on to this component.","title":"Training Your Own"},{"location":"docs/featurizer/gensim/#configurable-variables","text":"cache_dir : pass it the name of the directory where you've downloaded/saved the embeddings file : pass it the name of the .kv file that contains the word embeddings","title":"Configurable Variables"},{"location":"docs/featurizer/gensim/#base-usage","text":"The configuration file below demonstrates how you might use the gensim embeddings. In this example we're building a pipeline for the English language and we're assuming that you've trained your own embeddings which have been saved upfront as saved/beforehand/filename.kv . language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.GensimFeaturizer cache_dir : saved/beforehand file : filename.kv - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/semantic_map/","text":"The SemanticMapFeaturizer is an experimental sparse featurizer developed by Rasa. It can only be used in combination with pre-trained embedding files, which you can find here . Please refer to our blog posts for more details. Configurable Variables \u00b6 pretrained_semantic_map : Path to downloaded/saved semantic map embeddings (the unpacked json file) pooling : The pooling operation to use for the sentence features ( sum (default), mean , or merge ) Basic Usage \u00b6 The configuration file below demonstrates how you might use the semantic map embeddings. In this example we're building a pipeline for the English language and we're assuming that you've saved embeddings upfront as saved/beforehand/rasa-sme-wikipedia-en-64x64-v20201120.json . language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.featurizers.sparse.SemanticMapFeaturizer pretrained_semantic_map : \"saved/beforehand/rasa-sme-wikipedia64x64-en-v20201120.json\" - name : DIETClassifier epochs : 100","title":"SemanticMapFeaturizer"},{"location":"docs/featurizer/semantic_map/#configurable-variables","text":"pretrained_semantic_map : Path to downloaded/saved semantic map embeddings (the unpacked json file) pooling : The pooling operation to use for the sentence features ( sum (default), mean , or merge )","title":"Configurable Variables"},{"location":"docs/featurizer/semantic_map/#basic-usage","text":"The configuration file below demonstrates how you might use the semantic map embeddings. In this example we're building a pipeline for the English language and we're assuming that you've saved embeddings upfront as saved/beforehand/rasa-sme-wikipedia-en-64x64-v20201120.json . language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.featurizers.sparse.SemanticMapFeaturizer pretrained_semantic_map : \"saved/beforehand/rasa-sme-wikipedia64x64-en-v20201120.json\" - name : DIETClassifier epochs : 100","title":"Basic Usage"},{"location":"docs/featurizer/sparse_bytepair/","text":"This featurizer is a sparse featurizer. A main feature of these features is that you can use a pretrained BytePair tokeniser to encode the received text. Since BytePair embeddings exist for 277 languages all of which also have a pretrained tokeniser we figured it worth sharing. The tokeniser won't actually cause tokens to appear in Rasa. Rather, it applies a tokenisation trick before doing a countvector-encoding. \"talking about geology\" -> '\u2581talk ing \u2581about \u2581ge ology' Instead of encoding three words, this featurizer would encode 5 subtokens. Note that we also create features for each token. So the token \"talking\" will get the features for \"_talk\" and \"ing\". More information on the available models can be found here . When you scroll down you will notice a large of languages that are available. Here's some examples from that list that give a detailed view of available languages: Abkhazian Zulu English Hindi Chinese Esperanto Multi Language Configurable Variables \u00b6 lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25 , vs : specifies the vocabulary size of the segmentation model, default = 1000 , cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default= None , Base Usage \u00b6 The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. Also note that we recommend keeping the vocabulary size small if you're interested in spelling robustness as well as token featurization. Large vocabularies correspond with words being encoded as opposed to subwords. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.SparseBytePairFeaturizer lang : en vs : 1000 - name : DIETClassifier epochs : 100 Cached Usage \u00b6 If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 Note that in this case we expect a file to be present in the tests/data/en directory; en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file setting. But it is preferable to stick to the library naming convention. Also note that if you use the model_file setting that you must provide full filepaths and that the cache_dir will be ignored.","title":"SparseBytePairFeaturizer"},{"location":"docs/featurizer/sparse_bytepair/#configurable-variables","text":"lang : specifies the lanuage that you'll use, default = \"en\" dim : specifies the dimension of the subword embeddings, default = 25 , vs : specifies the vocabulary size of the segmentation model, default = 1000 , cache_dir : specifies the folder in which downloaded BPEmb files will be cached, default = ~/.cache/bpemb model_file : specifies the path to a custom model file, default= None ,","title":"Configurable Variables"},{"location":"docs/featurizer/sparse_bytepair/#base-usage","text":"The configuration file below demonstrates how you might use the BytePair embeddings. In this example we're not using any cached folders and the library will automatically download the correct embeddings for you and save them in ~/.cache . Both the embeddings as well as a model file will be saved. Also note that we recommend keeping the vocabulary size small if you're interested in spelling robustness as well as token featurization. Large vocabularies correspond with words being encoded as opposed to subwords. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.SparseBytePairFeaturizer lang : en vs : 1000 - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/featurizer/sparse_bytepair/#cached-usage","text":"If you're using pre-downloaded embedding files (in docker you might have this on a mounted disk) then you can prevent a download from happening. We'll be doing that in the example below. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.featurizers.dense.BytePairFeaturizer lang : en vs : 1000 cache_dir : \"tests/data\" - name : DIETClassifier epochs : 100 Note that in this case we expect a file to be present in the tests/data/en directory; en.wiki.bpe.vs10000.model You can also overwrite the names of these files via the model_file setting. But it is preferable to stick to the library naming convention. Also note that if you use the model_file setting that you must provide full filepaths and that the cache_dir will be ignored.","title":"Cached Usage"},{"location":"docs/jupyter/tools/","text":"This library also hosts some common tools to make it easier to investigate your training data or trained models from a jupyter notebook. RasaClassifier \u00b6 The RasaClassifier takes a pretrained Rasa model and turns it into a scikit-learn compatible estimator. It expects text as input and it will predict an intent class. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict ([ \"hello there\" , \"are you a bot?\" ]) fetch_info_from_message ( self , text_input ) \u00b6 Show source code in scikit/classifier.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def fetch_info_from_message ( self , text_input ): \"\"\" Fetch all the info from a single text input. Can be used to also retreive entities. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.fetch_info_from_message(\"hello there\") ``` \"\"\" return self . interpreter . interpreter . parse ( text_input ) Fetch all the info from a single text input. Can be used to also retreive entities. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . fetch_info_from_message ( \"hello there\" ) predict ( self , X ) \u00b6 Show source code in scikit/classifier.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def predict ( self , X ): \"\"\" Makes a class prediction, scikit-style. Note that we do not support `predict_proba` because our API allows for confidence values that do not lie in the [0, 1] range. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.predict([\"hello there\", \"are you a bot?\"]) ``` \"\"\" return np . array ([ self . fetch_info_from_message ( x )[ \"intent\" ][ \"name\" ] for x in X ]) Makes a class prediction, scikit-style. Note that we do not support predict_proba because our API allows for confidence values that do not lie in the [0, 1] range. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict ([ \"hello there\" , \"are you a bot?\" ]) dataframe_to_nlu_file ( dataf , write_path , text_col = 'text' , label_col = 'intent' ) \u00b6 Show source code in scikit/common.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def dataframe_to_nlu_file ( dataf , write_path , text_col = \"text\" , label_col = \"intent\" ): \"\"\" Converts a single DataFrame file with intents into a intents file for Rasa. Note that you cannot use this method to add entities. Usage: ```python import pandas as pd from rasa_nlu_examples.scikit import dataframe_to_nlu_file df = pd.DataFrame([ {\"text\": \"i really really like this\", \"intent\": \"positive\"}, {\"text\": \"i enjoy this\", \"intent\": \"positive\"}, {\"text\": \"this is not my thing\", \"intent\": \"negative\"} ]) dataframe_to_nlu_file(df, write_path=\"path/to/nlu.yml\") ``` This will yield a file with the following contents: ```yaml version: 2.0 nlu: - intent: negative examples: | - this is not my thing - intent: positive examples: | - i really really like this - i enjoy this ``` \"\"\" result = { \"version\" : str ( 2.0 ), \"nlu\" : []} for idx , group in dataf . groupby ( label_col ): intent = group [ label_col ] . iloc [ 0 ] result [ \"nlu\" ] . append ( { \"intent\" : intent , \"examples\" : [ t for t in group [ text_col ]], } ) dump = ( yaml . dump ( result , sort_keys = False , width = 1000 ) . replace ( \"examples:\" , \"examples: |\" ) . replace ( \" -\" , \" -\" ) ) return pathlib . Path ( write_path ) . write_text ( dump ) Converts a single DataFrame file with intents into a intents file for Rasa. Note that you cannot use this method to add entities. Usage: import pandas as pd from rasa_nlu_examples.scikit import dataframe_to_nlu_file df = pd . DataFrame ([ { \"text\" : \"i really really like this\" , \"intent\" : \"positive\" }, { \"text\" : \"i enjoy this\" , \"intent\" : \"positive\" }, { \"text\" : \"this is not my thing\" , \"intent\" : \"negative\" } ]) dataframe_to_nlu_file ( df , write_path = \"path/to/nlu.yml\" ) This will yield a file with the following contents: version : 2.0 nlu : - intent : negative examples : | - this is not my thing - intent : positive examples : | - i really really like this - i enjoy this nlu_path_to_dataframe ( path ) \u00b6 Show source code in scikit/common.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def nlu_path_to_dataframe ( path ): \"\"\" Converts a single nlu file with intents into a dataframe. Usage: ```python from rasa_nlu_examples.scikit import nlu_path_to_dataframe df = nlu_path_to_dataframe(\"path/to/nlu/nlu.yml\") ``` \"\"\" from rasa.nlu.convert import convert_training_data data = [] p = pathlib . Path ( path ) name = p . parts [ - 1 ] name = name [: name . find ( \".\" )] convert_training_data ( str ( p ), f \" { name } .json\" , output_format = \"json\" , language = \"en\" ) blob = json . loads ( pathlib . Path ( f \" { name } .json\" ) . read_text ()) for d in blob [ \"rasa_nlu_data\" ][ \"common_examples\" ]: data . append ({ \"text\" : d [ \"text\" ], \"label\" : d [ \"intent\" ]}) pathlib . Path ( f \" { name } .json\" ) . unlink () return pd . DataFrame ( data ) Converts a single nlu file with intents into a dataframe. Usage: from rasa_nlu_examples.scikit import nlu_path_to_dataframe df = nlu_path_to_dataframe ( \"path/to/nlu/nlu.yml\" )","title":"Overview"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.classifier.RasaClassifier","text":"The RasaClassifier takes a pretrained Rasa model and turns it into a scikit-learn compatible estimator. It expects text as input and it will predict an intent class. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict ([ \"hello there\" , \"are you a bot?\" ])","title":"RasaClassifier"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.classifier.RasaClassifier.fetch_info_from_message","text":"Show source code in scikit/classifier.py 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def fetch_info_from_message ( self , text_input ): \"\"\" Fetch all the info from a single text input. Can be used to also retreive entities. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.fetch_info_from_message(\"hello there\") ``` \"\"\" return self . interpreter . interpreter . parse ( text_input ) Fetch all the info from a single text input. Can be used to also retreive entities. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . fetch_info_from_message ( \"hello there\" )","title":"fetch_info_from_message()"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.classifier.RasaClassifier.predict","text":"Show source code in scikit/classifier.py 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def predict ( self , X ): \"\"\" Makes a class prediction, scikit-style. Note that we do not support `predict_proba` because our API allows for confidence values that do not lie in the [0, 1] range. Usage: ```python from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier(model_path=\"path/to/model.tar.gz\") mod.predict([\"hello there\", \"are you a bot?\"]) ``` \"\"\" return np . array ([ self . fetch_info_from_message ( x )[ \"intent\" ][ \"name\" ] for x in X ]) Makes a class prediction, scikit-style. Note that we do not support predict_proba because our API allows for confidence values that do not lie in the [0, 1] range. Usage: from rasa_nlu_examples.scikit import RasaClassifier mod = RasaClassifier ( model_path = \"path/to/model.tar.gz\" ) mod . predict ([ \"hello there\" , \"are you a bot?\" ])","title":"predict()"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.common.dataframe_to_nlu_file","text":"Show source code in scikit/common.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 def dataframe_to_nlu_file ( dataf , write_path , text_col = \"text\" , label_col = \"intent\" ): \"\"\" Converts a single DataFrame file with intents into a intents file for Rasa. Note that you cannot use this method to add entities. Usage: ```python import pandas as pd from rasa_nlu_examples.scikit import dataframe_to_nlu_file df = pd.DataFrame([ {\"text\": \"i really really like this\", \"intent\": \"positive\"}, {\"text\": \"i enjoy this\", \"intent\": \"positive\"}, {\"text\": \"this is not my thing\", \"intent\": \"negative\"} ]) dataframe_to_nlu_file(df, write_path=\"path/to/nlu.yml\") ``` This will yield a file with the following contents: ```yaml version: 2.0 nlu: - intent: negative examples: | - this is not my thing - intent: positive examples: | - i really really like this - i enjoy this ``` \"\"\" result = { \"version\" : str ( 2.0 ), \"nlu\" : []} for idx , group in dataf . groupby ( label_col ): intent = group [ label_col ] . iloc [ 0 ] result [ \"nlu\" ] . append ( { \"intent\" : intent , \"examples\" : [ t for t in group [ text_col ]], } ) dump = ( yaml . dump ( result , sort_keys = False , width = 1000 ) . replace ( \"examples:\" , \"examples: |\" ) . replace ( \" -\" , \" -\" ) ) return pathlib . Path ( write_path ) . write_text ( dump ) Converts a single DataFrame file with intents into a intents file for Rasa. Note that you cannot use this method to add entities. Usage: import pandas as pd from rasa_nlu_examples.scikit import dataframe_to_nlu_file df = pd . DataFrame ([ { \"text\" : \"i really really like this\" , \"intent\" : \"positive\" }, { \"text\" : \"i enjoy this\" , \"intent\" : \"positive\" }, { \"text\" : \"this is not my thing\" , \"intent\" : \"negative\" } ]) dataframe_to_nlu_file ( df , write_path = \"path/to/nlu.yml\" ) This will yield a file with the following contents: version : 2.0 nlu : - intent : negative examples : | - this is not my thing - intent : positive examples : | - i really really like this - i enjoy this","title":"dataframe_to_nlu_file()"},{"location":"docs/jupyter/tools/#rasa_nlu_examples.scikit.common.nlu_path_to_dataframe","text":"Show source code in scikit/common.py 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 def nlu_path_to_dataframe ( path ): \"\"\" Converts a single nlu file with intents into a dataframe. Usage: ```python from rasa_nlu_examples.scikit import nlu_path_to_dataframe df = nlu_path_to_dataframe(\"path/to/nlu/nlu.yml\") ``` \"\"\" from rasa.nlu.convert import convert_training_data data = [] p = pathlib . Path ( path ) name = p . parts [ - 1 ] name = name [: name . find ( \".\" )] convert_training_data ( str ( p ), f \" { name } .json\" , output_format = \"json\" , language = \"en\" ) blob = json . loads ( pathlib . Path ( f \" { name } .json\" ) . read_text ()) for d in blob [ \"rasa_nlu_data\" ][ \"common_examples\" ]: data . append ({ \"text\" : d [ \"text\" ], \"label\" : d [ \"intent\" ]}) pathlib . Path ( f \" { name } .json\" ) . unlink () return pd . DataFrame ( data ) Converts a single nlu file with intents into a dataframe. Usage: from rasa_nlu_examples.scikit import nlu_path_to_dataframe df = nlu_path_to_dataframe ( \"path/to/nlu/nlu.yml\" )","title":"nlu_path_to_dataframe()"},{"location":"docs/meta/printer/","text":"Here's an example configuration file that demonstrates how the custom printer component works. Configurable Variables \u00b6 alias : gives an extra name to the component and adds an extra message that is printed Base Usage \u00b6 When running this example you'll notice that what the effect is of the CountVectorsFeaturizer . You should see print statements appear when you talk to the assistant. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.meta.Printer alias : before count vectors - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.meta.Printer alias : after count vectors - name : DIETClassifier epochs : 100 When you now interact with your model via rasa shell you will see pretty information appear about the state of the Message object. It might look something like this; .r1 {font-weight: bold} .r2 {color: #008000} .r3 {color: #000080; font-weight: bold} { 'text' : 'rasa nlu examples' , 'intent' : { 'name' : 'out_of_scope' , 'confidence' : 0.4313829839229584 } , 'entities' : [ { 'entity' : 'proglang' , 'start' : 0 , 'end' : 4 , 'confidence_entity' : 0.42326217889785767 , 'value' : 'rasa' , 'extractor' : 'DIETClassifier' } ] , 'text_tokens' : [ 'rasa' , 'nlu' , 'examples' ] , 'intent_ranking' : [ { 'name' : 'out_of_scope' , 'confidence' : 0.4313829839229584 } , { 'name' : 'goodbye' , 'confidence' : 0.2445288747549057 } , { 'name' : 'bot_challenge' , 'confidence' : 0.23958507180213928 } , { 'name' : 'greet' , 'confidence' : 0.04896979033946991 } , { 'name' : 'talk_code' , 'confidence' : 0.035533301532268524 } ] , 'dense' : { 'sequence' : { 'shape' : ( 3 , 25 ) , 'dtype' : dtype ( 'float32' )} , 'sentence' : { 'shape' : ( 1 , 25 ) , 'dtype' : dtype ( 'float32' )} } , 'sparse' : { 'sequence' : { 'shape' : ( 3 , 1780 ) , 'dtype' : dtype ( 'float64' ) , 'stored_elements' : 67 } , 'sentence' : { 'shape' : ( 1 , 1756 ) , 'dtype' : dtype ( 'int64' ) , 'stored_elements' : 32 } } }","title":"Printer"},{"location":"docs/meta/printer/#configurable-variables","text":"alias : gives an extra name to the component and adds an extra message that is printed","title":"Configurable Variables"},{"location":"docs/meta/printer/#base-usage","text":"When running this example you'll notice that what the effect is of the CountVectorsFeaturizer . You should see print statements appear when you talk to the assistant. language : en pipeline : - name : WhitespaceTokenizer - name : LexicalSyntacticFeaturizer - name : rasa_nlu_examples.meta.Printer alias : before count vectors - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : rasa_nlu_examples.meta.Printer alias : after count vectors - name : DIETClassifier epochs : 100 When you now interact with your model via rasa shell you will see pretty information appear about the state of the Message object. It might look something like this; .r1 {font-weight: bold} .r2 {color: #008000} .r3 {color: #000080; font-weight: bold} { 'text' : 'rasa nlu examples' , 'intent' : { 'name' : 'out_of_scope' , 'confidence' : 0.4313829839229584 } , 'entities' : [ { 'entity' : 'proglang' , 'start' : 0 , 'end' : 4 , 'confidence_entity' : 0.42326217889785767 , 'value' : 'rasa' , 'extractor' : 'DIETClassifier' } ] , 'text_tokens' : [ 'rasa' , 'nlu' , 'examples' ] , 'intent_ranking' : [ { 'name' : 'out_of_scope' , 'confidence' : 0.4313829839229584 } , { 'name' : 'goodbye' , 'confidence' : 0.2445288747549057 } , { 'name' : 'bot_challenge' , 'confidence' : 0.23958507180213928 } , { 'name' : 'greet' , 'confidence' : 0.04896979033946991 } , { 'name' : 'talk_code' , 'confidence' : 0.035533301532268524 } ] , 'dense' : { 'sequence' : { 'shape' : ( 3 , 25 ) , 'dtype' : dtype ( 'float32' )} , 'sentence' : { 'shape' : ( 1 , 25 ) , 'dtype' : dtype ( 'float32' )} } , 'sparse' : { 'sequence' : { 'shape' : ( 3 , 1780 ) , 'dtype' : dtype ( 'float64' ) , 'stored_elements' : 67 } , 'sentence' : { 'shape' : ( 1 , 1756 ) , 'dtype' : dtype ( 'int64' ) , 'stored_elements' : 32 } } }","title":"Base Usage"},{"location":"docs/tokenizer/spacy_tokenizer/","text":"Rasa natively supports spaCy models that have a language model attached. But spaCy also offers tokenizers without a model. We support these tokenisers with this component. Note In order to use this tool you'll need to ensure that spaCy is installed with Rasa. pip install rasa[spacy] You should also be aware that for certain languages extra dependencies are required. More information is given on the spacy documentation . Configurable Variables \u00b6 lang : the two-letter abbreviation of the language you want to use. Base Usage \u00b6 Once downloaded it can be used in a Rasa configuration, like below; language : en pipeline : - name : rasa_nlu_examples.tokenizers.BlankSpacyTokenizer lang : \"en\" - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100","title":"BlankSpacyTokenizer"},{"location":"docs/tokenizer/spacy_tokenizer/#configurable-variables","text":"lang : the two-letter abbreviation of the language you want to use.","title":"Configurable Variables"},{"location":"docs/tokenizer/spacy_tokenizer/#base-usage","text":"Once downloaded it can be used in a Rasa configuration, like below; language : en pipeline : - name : rasa_nlu_examples.tokenizers.BlankSpacyTokenizer lang : \"en\" - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100","title":"Base Usage"},{"location":"docs/tokenizer/stanza/","text":"The Stanza project from Stanford supports tokenizers, lemmatizers as well as part of speech detection for many languages that are not supported by spaCy. You can find the available languages here . Note In order to use this tool you'll need to ensure the correct dependencies are installed. pip install \"rasa_nlu_examples[stanza] @ https://github.com/RasaHQ/rasa-nlu-examples.git\" Model Download \u00b6 To use a Stanza model you'll first need to download it. This can be done from python. import stanza # download English model in the ~/stanza_resources dir stanza . download ( 'en' , dir = '~/stanza_resources' ) Configurable Variables \u00b6 lang : then two-letter abbreprivation of the language you want to use cache_dir : pass it the name of the directory where you've downloaded/saved the embeddings Base Usage \u00b6 Once downloaded it can be used in a Rasa configuration, like below; language : en pipeline : - name : rasa_nlu_examples.tokenizers.StanzaTokenizer lang : \"en\" cache_dir : \"~/stanza_resources\" - name : LexicalSyntacticFeaturizer \"features\" : [ [ \"low\" , \"title\" , \"upper\" ], [ \"BOS\" , \"EOS\" , \"low\" , \"upper\" , \"title\" , \"digit\" , \"pos\" ], [ \"low\" , \"title\" , \"upper\" ], ] - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 One thing to note here is that the LexicalSyntacticFeaturizer will be able to pick up the \"pos\" information with the StanzaTokenizer just like you're able to do that with spaCy. The CountVectorsFeaturizer is able to pick up the lemma features that are generated.","title":"StanzaTokenizer"},{"location":"docs/tokenizer/stanza/#model-download","text":"To use a Stanza model you'll first need to download it. This can be done from python. import stanza # download English model in the ~/stanza_resources dir stanza . download ( 'en' , dir = '~/stanza_resources' )","title":"Model Download"},{"location":"docs/tokenizer/stanza/#configurable-variables","text":"lang : then two-letter abbreprivation of the language you want to use cache_dir : pass it the name of the directory where you've downloaded/saved the embeddings","title":"Configurable Variables"},{"location":"docs/tokenizer/stanza/#base-usage","text":"Once downloaded it can be used in a Rasa configuration, like below; language : en pipeline : - name : rasa_nlu_examples.tokenizers.StanzaTokenizer lang : \"en\" cache_dir : \"~/stanza_resources\" - name : LexicalSyntacticFeaturizer \"features\" : [ [ \"low\" , \"title\" , \"upper\" ], [ \"BOS\" , \"EOS\" , \"low\" , \"upper\" , \"title\" , \"digit\" , \"pos\" ], [ \"low\" , \"title\" , \"upper\" ], ] - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 One thing to note here is that the LexicalSyntacticFeaturizer will be able to pick up the \"pos\" information with the StanzaTokenizer just like you're able to do that with spaCy. The CountVectorsFeaturizer is able to pick up the lemma features that are generated.","title":"Base Usage"},{"location":"docs/tokenizer/thai_tokenizer/","text":"The ThaiTokenizer is a Rasa compatible tokenizer for Thai, using PyThaiNLP under the hood. In order to use the ThaiTokenizer the language must be set to th - no other languages are supported by this tokenizer. Note In order to use this tool you'll need to ensure the correct dependencies are installed. pip install \"rasa_nlu_examples[thai] @ https://github.com/RasaHQ/rasa-nlu-examples.git\" Configurable Variables \u00b6 None Base Usage \u00b6 The ThaiTokenizer can be used in a Rasa configuration like below: language : th pipeline : - name : rasa_nlu_examples.tokenizers.ThaiTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 If there are any issues with this tokenizer, please let us know .","title":"ThaiTokenizer"},{"location":"docs/tokenizer/thai_tokenizer/#configurable-variables","text":"None","title":"Configurable Variables"},{"location":"docs/tokenizer/thai_tokenizer/#base-usage","text":"The ThaiTokenizer can be used in a Rasa configuration like below: language : th pipeline : - name : rasa_nlu_examples.tokenizers.ThaiTokenizer - name : CountVectorsFeaturizer - name : CountVectorsFeaturizer analyzer : char_wb min_ngram : 1 max_ngram : 4 - name : DIETClassifier epochs : 100 If there are any issues with this tokenizer, please let us know .","title":"Base Usage"}]}